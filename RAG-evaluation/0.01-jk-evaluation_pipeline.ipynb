{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce38b2be",
   "metadata": {},
   "source": [
    "## Setting up an Evaluation Pipeline for Research Assistant v2\n",
    "- Utilize Llama Index Library\n",
    "- Switch out LLM and Embeddings Model\n",
    "\n",
    "**Useful Links**\n",
    "\n",
    "**Llama Index RAG Implementation**\n",
    "- [Starter] https://medium.com/mitb-for-all/a-gentle-introduction-to-the-llm-multiverse-part-3-llamaindex-798344050c49\n",
    "- [Docs] Basic and Advanced RAG systems https://www.llamaindex.ai/blog/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b \n",
    "- [Docs] Customising LLMs within LlamaIndex Abstractions https://docs.llamaindex.ai/en/stable/module_guides/models/llms/usage_custom/ \n",
    "- [Docs] Customising Embedding Model for Vector store Indexing https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/ \n",
    "\n",
    "**Evaluation Framework**\n",
    "- [Docs] https://docs.llamaindex.ai/en/stable/examples/cookbooks/oreilly_course_cookbooks/Module-3/Evaluating_RAG_Systems/#correctness-evaluator\n",
    "- [Ragas] https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_relevance/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eda6522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set autoreload for modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import dependencies\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import warnings\n",
    "import nest_asyncio\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966b51c2",
   "metadata": {},
   "source": [
    "#### Developing Evaluation Dataset (with question-answer pairs relating to context)\n",
    "**1. *First, let us load configure the LLM and Custom Embedding model  for our RAG system - “hkunlp/instructor-large”***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dd434f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    Settings,\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Configure LLM\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d800cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use custom embedding model - “hkunlp/instructor-large”\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# load embedding model (try) - loads https://huggingface.co/hkunlp/instructor-large\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"hkunlp/instructor-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4edc44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "[-0.048856671899557114, -0.00874248519539833, -0.013273396529257298, -0.011948740109801292, 0.020680604502558708]\n"
     ]
    }
   ],
   "source": [
    "# Test embedding model\n",
    "embeddings = embed_model.get_text_embedding(\"Hello World!\")\n",
    "print(len(embeddings))\n",
    "print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f547ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set embedding model\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a4465e",
   "metadata": {},
   "source": [
    "**2. Next, *Ingest documents/context and generate RAG evaluation dataset.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dae4e861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 1/1 [00:07<00:00,  7.55s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator\n",
    "\n",
    "# load documents from pdf format\n",
    "docs = SimpleDirectoryReader(\"../RAG-webscraper/docs/\").load_data(show_progress=True)\n",
    "\n",
    "# input llm and docs to instantiate RAG dataset generator\n",
    "data_gen = RagDatasetGenerator.from_documents(\n",
    "    docs,\n",
    "    llm= Settings.llm,\n",
    "    question_gen_query=\"You are a teacher/professor. Using the provided context, formulat a single question and its answer\",\n",
    "    num_questions_per_chunk=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d938ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run generate RAG dataset\n",
    "rag_dataset = data_gen.generate_dataset_from_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e36c97ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelledRagDataset(examples=[LabelledRagDataExample(query='**Question:** Who are the members of Group 9 in the context of the eyecatcher project report?', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['Group 9 \\nChristover Abraham Manafe \\nLoh Kwang Peng Micheal \\nLow Siang Leng Henry \\nYee Jin Kett \\nAEYECATCHER.PY \\nCS611 - Machine Learning Engineering'], reference_answer='The members of Group 9 in the context of the eyecatcher project report are:\\n\\n1. Christover Abraham Manafe\\n2. Loh Kwang Peng Micheal\\n3. Low Siang Leng Henry\\n4. Yee Jin Kett', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Answer:** The members of Group 9 are Christover Abraham Manafe, Loh Kwang Peng Micheal, Low Siang Leng Henry, and Yee Jin Kett.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['Group 9 \\nChristover Abraham Manafe \\nLoh Kwang Peng Micheal \\nLow Siang Leng Henry \\nYee Jin Kett \\nAEYECATCHER.PY \\nCS611 - Machine Learning Engineering'], reference_answer='The members of Group 9 are Christover Abraham Manafe, Loh Kwang Peng Micheal, Low Siang Leng Henry, and Yee Jin Kett.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Question:** What are the key components of the machine learning system architecture as outlined in the project report?', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['1 \\nTable of Contents \\n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \\n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \\n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \\n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \\n1.4. STAKEHOLDERS – USERS & ADMINISTRATORS ...................................................................................................... 2 \\n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \\n2.1. DATA SOURCES ............................................................................................................................................. 3 \\n2.2. DATA LABELLING............................................................................................................................................ 3 \\n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \\n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \\n2.5. DATASET FORMAT .......................................................................................................................................... 3 \\n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \\n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \\n3.1. MODEL BUILDING .......................................................................................................................................... 4 \\n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \\n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \\n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \\n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \\n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \\n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \\n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \\n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \\n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \\n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \\n3.3.2. IMPLEMENTATION OF USER FEEDBACK – DISCORD SERVER BOT ................................................................................ 7 \\n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \\n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \\n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \\n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \\n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \\n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \\n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \\n5. REFERENCES ............................................................................................................................................... 10 \\n6. APPENDIX .................................................................................................................................................. 11'], reference_answer='The key components of the machine learning system architecture outlined in the project report are:\\n\\n1. **Model Building**\\n   - Model Building Workflow\\n   - Data Preprocessing\\n   - Model Training\\n   - Model Quantization\\n\\n2. **Model Deployment**\\n   - Model Deployment Workflow\\n   - Auto Scaling Policy\\n   - Deployment Strategy\\n\\n3. **Monitoring & Retraining Step**\\n   - User Feedback to Handle Concept & Model Drift\\n   - Implementation of User Feedback – Discord Server Bot', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Answer:** The key components of the machine learning system architecture outlined in the project report include model building, model deployment, and monitoring & retraining steps. Specifically, model building encompasses the workflow, data preprocessing, model training, and model quantization. Model deployment includes the deployment workflow, auto scaling policy, and deployment strategy. Finally, the monitoring & retraining step involves user feedback to handle concept and model drift, as well as the implementation of user feedback through a Discord server bot.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['1 \\nTable of Contents \\n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \\n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \\n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \\n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \\n1.4. STAKEHOLDERS – USERS & ADMINISTRATORS ...................................................................................................... 2 \\n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \\n2.1. DATA SOURCES ............................................................................................................................................. 3 \\n2.2. DATA LABELLING............................................................................................................................................ 3 \\n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \\n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \\n2.5. DATASET FORMAT .......................................................................................................................................... 3 \\n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \\n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \\n3.1. MODEL BUILDING .......................................................................................................................................... 4 \\n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \\n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \\n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \\n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \\n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \\n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \\n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \\n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \\n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \\n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \\n3.3.2. IMPLEMENTATION OF USER FEEDBACK – DISCORD SERVER BOT ................................................................................ 7 \\n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \\n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \\n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \\n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \\n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \\n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \\n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \\n5. REFERENCES ............................................................................................................................................... 10 \\n6. APPENDIX .................................................................................................................................................. 11'], reference_answer='The key components of the machine learning system architecture outlined in the project report include:\\n\\n1. **Model Building**:\\n   - Workflow\\n   - Data Preprocessing\\n   - Model Training\\n   - Model Quantization\\n\\n2. **Model Deployment**:\\n   - Deployment Workflow\\n   - Auto Scaling Policy\\n   - Deployment Strategy\\n\\n3. **Monitoring & Retraining Steps**:\\n   - User Feedback to Handle Concept and Model Drift\\n   - Implementation of User Feedback through a Discord Server Bot', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Question:** What is the primary focus of the project described in the report, and how does it aim to address the challenges of content moderation on social media platforms?', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=[\"2 \\n1. Business Problem & Implications  \\n1.1. Problem Background \\nIn today's digital age, social media platforms and website s have become an integral part of our lives, and the \\namount of content being shared and uploaded online is increasing exponentially. With the rise in popularity of \\nsocial media platforms like TikTok, Instagram, and Facebook, the need for explicit/NSFW1 image moderation has \\nbecome more important than ever. With 3.2 billion images and 720,000 videos being shared daily (T.J. Thomson \\net.al, 2022), this has given rise to the complexity of content moderation. Content moderation is an industry-wide \\nproblem as cited by TikTok CEO Chew Shou Zi, and it is tough to identify and take down objectionable contents2 \\nsuch as suggestive content, violence, vices & racial slurs in a timely manner. \\nWhile social m edia giants like Facebook and TikTok have invested heavily in machi ne learning and human \\nmoderators to conduct moderation activity to remove unsafe content from their respective platforms, start-ups \\nand SMEs are unable to employ the similar processes due to budgetary constraints. \\n1.2. Project Focus \\nOur project aims to val ue add to this field by develop ing a deployable machine learning pipeline for explicit \\nimage classification, with a particular focus on explicit nudity detection.  \\n1.3. Project Scope \\nWe pla n to use state -of-the-art machine learning algorithms and technique s to develop a model that can \\naccurately detect and filter out explicit images, including, but not limited to nudity and sexual exposure. Through \\nthis, businesses can leverage on a powerful  yet cost-effective tool to moderate the content on their platforms , \\nenabling users’ trust and safety while maintaining brand reputation. \\nSubsequently, we would develop a cloud-native solution by leveragin g on services such as Amazon SageMaker \\nand AWS Lambda that is highly tailored to the business’ needs.  \\n1.4. Stakeholders – Users & Administrators \\nStakeholders, including both users and administrators, can leverage our machine learning system in various ways \\nto enhance their experience and ensure a safer online environment. \\nUsers of social media platforms will upload images a nd receive feedback on their  contents from the pipeline. \\nThis feedback will indicate if the image contains explicit nudity or not. A dditionally, users can perform self -\\nlabelling by repor ting inappropriate images (in situations where the ML system fail to f lag out inappropriate \\nimages). When a certain threshold of reported images is reached, the system will trigger a model retraining to  \\nimprove the accuracy of the pipeline's explicit image classification over time.  \\nOn the other hand, social media community managers will be the primary  administrators of our machine \\nlearning system. They will be responsible for maintaining the pipeline's functionality and ensuring the accuracy \\nand reliability of the system. As part of their role, they will monitor the pipeline 's performance, fine -tune the \\nsystem parameters, and carry out periodic updates to the model. By utilizing our ML system, administrators can \\nfocus their efforts on managing the platform and creating a seamless user experience, while having confidence \\nin the system's ability to enhance content moderation and foster a safer online community. \\nExisting use-cases suggest that community mana gers often face the challenge of moderating user -generated \\ncontent in real-time. To tackle this challenge, some companies ha ve implemented machine learning systems to \\nhelp identify inappropriate content and flag them for review. Our machine learning system  aims to provide a \\nsimilar solution that can effective ly support social media community managers in monitoring user -generated \\ncontent for explicit nudity. By leveraging self-labelling features, the system can also promote user engagement \\nand foster a safer online community. Overall, our ML system offers stakeholders a comprehensive solution that \\nfacilitates content moderation, empowers user engagement, an d ultimately contributes to a more responsible \\nand respectful online environment. \\n \\n1 Not safe for work \\n2 See Appendix: Figure A for common categories of content moderation on Social Media platforms\"], reference_answer=\"The primary focus of the project described in the report is to develop a deployable machine learning pipeline for explicit image classification, specifically targeting explicit nudity detection. The project aims to address the challenges of content moderation on social media platforms by creating a cost-effective tool that can accurately detect and filter out explicit images, including nudity and sexual exposure. \\n\\nBy leveraging state-of-the-art machine learning algorithms and techniques, the project intends to provide businesses, particularly start-ups and SMEs that may lack the resources of larger companies, with a powerful solution for moderating user-generated content. The system will allow users to receive feedback on their uploaded images regarding explicit content and enable self-labelling through reporting inappropriate images. This feedback mechanism will help improve the model's accuracy over time through retraining based on user reports.\\n\\nAdditionally, social media community managers will benefit from the system by having a reliable tool to assist in real-time content moderation, allowing them to focus on managing the platform and enhancing user experience. Overall, the project seeks to foster a safer online environment and maintain brand reputation by effectively supporting content moderation efforts.\", reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Answer:** The primary focus of the project is to develop a deployable machine learning pipeline for explicit image classification, specifically targeting explicit nudity detection. It aims to address the challenges of content moderation on social media platforms by providing a cost-effective tool that can accurately detect and filter out explicit images, thereby enhancing user trust and safety while maintaining brand reputation.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=[\"2 \\n1. Business Problem & Implications  \\n1.1. Problem Background \\nIn today's digital age, social media platforms and website s have become an integral part of our lives, and the \\namount of content being shared and uploaded online is increasing exponentially. With the rise in popularity of \\nsocial media platforms like TikTok, Instagram, and Facebook, the need for explicit/NSFW1 image moderation has \\nbecome more important than ever. With 3.2 billion images and 720,000 videos being shared daily (T.J. Thomson \\net.al, 2022), this has given rise to the complexity of content moderation. Content moderation is an industry-wide \\nproblem as cited by TikTok CEO Chew Shou Zi, and it is tough to identify and take down objectionable contents2 \\nsuch as suggestive content, violence, vices & racial slurs in a timely manner. \\nWhile social m edia giants like Facebook and TikTok have invested heavily in machi ne learning and human \\nmoderators to conduct moderation activity to remove unsafe content from their respective platforms, start-ups \\nand SMEs are unable to employ the similar processes due to budgetary constraints. \\n1.2. Project Focus \\nOur project aims to val ue add to this field by develop ing a deployable machine learning pipeline for explicit \\nimage classification, with a particular focus on explicit nudity detection.  \\n1.3. Project Scope \\nWe pla n to use state -of-the-art machine learning algorithms and technique s to develop a model that can \\naccurately detect and filter out explicit images, including, but not limited to nudity and sexual exposure. Through \\nthis, businesses can leverage on a powerful  yet cost-effective tool to moderate the content on their platforms , \\nenabling users’ trust and safety while maintaining brand reputation. \\nSubsequently, we would develop a cloud-native solution by leveragin g on services such as Amazon SageMaker \\nand AWS Lambda that is highly tailored to the business’ needs.  \\n1.4. Stakeholders – Users & Administrators \\nStakeholders, including both users and administrators, can leverage our machine learning system in various ways \\nto enhance their experience and ensure a safer online environment. \\nUsers of social media platforms will upload images a nd receive feedback on their  contents from the pipeline. \\nThis feedback will indicate if the image contains explicit nudity or not. A dditionally, users can perform self -\\nlabelling by repor ting inappropriate images (in situations where the ML system fail to f lag out inappropriate \\nimages). When a certain threshold of reported images is reached, the system will trigger a model retraining to  \\nimprove the accuracy of the pipeline's explicit image classification over time.  \\nOn the other hand, social media community managers will be the primary  administrators of our machine \\nlearning system. They will be responsible for maintaining the pipeline's functionality and ensuring the accuracy \\nand reliability of the system. As part of their role, they will monitor the pipeline 's performance, fine -tune the \\nsystem parameters, and carry out periodic updates to the model. By utilizing our ML system, administrators can \\nfocus their efforts on managing the platform and creating a seamless user experience, while having confidence \\nin the system's ability to enhance content moderation and foster a safer online community. \\nExisting use-cases suggest that community mana gers often face the challenge of moderating user -generated \\ncontent in real-time. To tackle this challenge, some companies ha ve implemented machine learning systems to \\nhelp identify inappropriate content and flag them for review. Our machine learning system  aims to provide a \\nsimilar solution that can effective ly support social media community managers in monitoring user -generated \\ncontent for explicit nudity. By leveraging self-labelling features, the system can also promote user engagement \\nand foster a safer online community. Overall, our ML system offers stakeholders a comprehensive solution that \\nfacilitates content moderation, empowers user engagement, an d ultimately contributes to a more responsible \\nand respectful online environment. \\n \\n1 Not safe for work \\n2 See Appendix: Figure A for common categories of content moderation on Social Media platforms\"], reference_answer='The primary focus of the project is to develop a deployable machine learning pipeline for explicit image classification, specifically targeting explicit nudity detection. It aims to address the challenges of content moderation on social media platforms by providing a cost-effective tool that can accurately detect and filter out explicit images, thereby enhancing user trust and safety while maintaining brand reputation.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Question:** What method did the team use for data labeling in their explicit image classification project, and what were the advantages of this method?', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['3 \\n2. Data Collection & Project Datasets \\n2.1. Data Sources \\nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \\nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \\nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \\nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition\\'s \\n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \\ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \\nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \\nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \\nuses human labellers. \\n2.2. Data Labelling \\nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \\nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \\nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \\nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition\\'s \\n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \\nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \\nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \\nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \\noffenders why the images are classified NSFW (“Borderline Nudity”) so as opposed to a Safe/NSFW classification. \\nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \\nresilient against future content drifts. \\n2.3. Dataset Statistics \\nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \\n2.4. Data Imbalance \\nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \\nin our data preprocessing step.  \\n2.5. Dataset Format \\nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \\nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \\nengineering, model training, tuning, evaluation, deployment, and monitoring.  \\nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \\nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \\nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \\nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \\nlearning. \\n2.6. Data Privacy & Ethical Considerations \\nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \\nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \\nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \\nwill limit access to the data to only those team members who requir e it for the project, and we will store the \\ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \\nused for any malicious or harmful purposes and that the project\\'s end goal serves a legitimate purpose in society. \\nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \\n \\n3 See Appendix: Figure B for the Data Collection Pipeline \\n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition'], reference_answer='The team used Amazon Rekognition\\'s \"DetectModerationLabels\" method for data labeling in their explicit image classification project. The advantages of this method included:\\n\\n1. **Cost-Effectiveness**: Using Amazon Rekognition, which employs an existing trained model for classification, was more cost-effective compared to Amazon SageMaker Ground Truth, which relies on human labelers.\\n\\n2. **Improved Data Quality**: The method allowed the team to generate multiple sub-classes/labels, enhancing the granularity of their dataset and improving the quality of the data used for training, validation, and testing.\\n\\n3. **Efficiency**: By leveraging pre-labeled datasets and automating the labeling process, the team minimized labeling costs and time, allowing for a more streamlined data collection process.\\n\\n4. **Resilience Against Content Drifts**: The use of detailed sub-labels (e.g., \"Borderline Nudity\") helps inform community managers and users about the classification, which can contribute to the model\\'s resilience against future content drifts.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Answer:** The team used Amazon Rekognition\\'s \"DetectModerationLabels\" method for data labeling. The advantages of this method included improved quality of the dataset for training, validation, and testing while minimizing labeling costs. Additionally, it utilized an existing trained model for classification, making it more cost-effective compared to using human labelers through Amazon SageMaker Ground Truth.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['3 \\n2. Data Collection & Project Datasets \\n2.1. Data Sources \\nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \\nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \\nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \\nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition\\'s \\n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \\ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \\nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \\nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \\nuses human labellers. \\n2.2. Data Labelling \\nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \\nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \\nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \\nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition\\'s \\n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \\nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \\nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \\nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \\noffenders why the images are classified NSFW (“Borderline Nudity”) so as opposed to a Safe/NSFW classification. \\nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \\nresilient against future content drifts. \\n2.3. Dataset Statistics \\nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \\n2.4. Data Imbalance \\nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \\nin our data preprocessing step.  \\n2.5. Dataset Format \\nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \\nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \\nengineering, model training, tuning, evaluation, deployment, and monitoring.  \\nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \\nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \\nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \\nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \\nlearning. \\n2.6. Data Privacy & Ethical Considerations \\nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \\nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \\nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \\nwill limit access to the data to only those team members who requir e it for the project, and we will store the \\ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \\nused for any malicious or harmful purposes and that the project\\'s end goal serves a legitimate purpose in society. \\nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \\n \\n3 See Appendix: Figure B for the Data Collection Pipeline \\n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition'], reference_answer='The team used Amazon Rekognition\\'s \"DetectModerationLabels\" method for data labeling. The advantages of this method included improved quality of the dataset for training, validation, and testing while minimizing labeling costs. Additionally, it utilized an existing trained model for classification, making it more cost-effective compared to using human labelers through Amazon SageMaker Ground Truth.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Question:** What are the main components of the machine learning system architecture described in the document, and what is the purpose of each component?', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['4 \\npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \\ncommunication to our stakeholders on how the data will be used an d protected. \\n3. Machine Learning System Architecture \\nThe team has segmented the pipeline into a few components for the purpose of the project5.  \\nService Explanation \\nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \\nfor model training \\nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \\ntake appropriate action. \\nAmazon SageMaker – Training Conduct model training, building and compilation \\nAmazon SageMaker – Model Registry Contains a catalogue of models to track and manage \\nAmazon SageMaker - Endpoint Deploying model for real-time inference \\nAWS CodeCommit Store source code and changes history  \\nAWS CodeBuild Compiles source code and build model  \\nAWS CodePipeline Automate pipeline for CI/CD \\nAmazon CloudWatch Monitor model performance from logs and to send alarm \\nAWS Lambda Serverless computing service to perform inference, update data \\nlabel, and trigger model training pipeline. \\nAPI Gateway Managed s ervice that facilitates interactions between public \\nrequests to AWS services. \\n \\n3.1. Model Building6 \\n3.1.1. Model Building Workflow7 \\nIn the development of the model, the team implemented a continuous integration approach. The \\ncommencement of this process is signaled when the model building code is committed into the repository. This \\nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \\nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \\nare listed as follows: \\n1. The preprocessing of training data. \\n2. The actual training of the model. \\n3. The evaluation of the model. \\n4. The final step, which involves registering the model into the Model Registry.  \\nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \\nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \\nbaseline performance standards for the model. \\n3.1.2. Data Preprocessing \\nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \\nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \\naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \\ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \\nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \\nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \\nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \\n \\n5 See Appendix: Figure E for the final system architecture. \\n6 See Appendix: Figure F for the detailed model building architecture. \\n7 See Appendix: Figure G for the CodePipeline stages for model building.'], reference_answer='The main components of the machine learning system architecture described in the document and their purposes are as follows:\\n\\n1. **Amazon S3 Training Bucket**: Stores training images that will be converted into PyTorch Tensors for model training.\\n\\n2. **Amazon S3 Interim Bucket**: Stores reported/appealed images for moderators to evaluate and take appropriate action.\\n\\n3. **Amazon SageMaker – Training**: Conducts model training, building, and compilation.\\n\\n4. **Amazon SageMaker – Model Registry**: Contains a catalogue of models to track and manage them.\\n\\n5. **Amazon SageMaker - Endpoint**: Deploys the model for real-time inference.\\n\\n6. **AWS CodeCommit**: Stores source code and maintains a history of changes.\\n\\n7. **AWS CodeBuild**: Compiles source code and builds the model.\\n\\n8. **AWS CodePipeline**: Automates the pipeline for Continuous Integration/Continuous Deployment (CI/CD).\\n\\n9. **Amazon CloudWatch**: Monitors model performance from logs and sends alarms.\\n\\n10. **AWS Lambda**: Provides serverless computing services to perform inference, update data labels, and trigger the model training pipeline.\\n\\n11. **API Gateway**: A managed service that facilitates interactions between public requests and AWS services. \\n\\nThese components work together to create a robust machine learning pipeline, from data storage and model training to deployment and monitoring.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Answer:** The main components of the machine learning system architecture include:', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['4 \\npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \\ncommunication to our stakeholders on how the data will be used an d protected. \\n3. Machine Learning System Architecture \\nThe team has segmented the pipeline into a few components for the purpose of the project5.  \\nService Explanation \\nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \\nfor model training \\nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \\ntake appropriate action. \\nAmazon SageMaker – Training Conduct model training, building and compilation \\nAmazon SageMaker – Model Registry Contains a catalogue of models to track and manage \\nAmazon SageMaker - Endpoint Deploying model for real-time inference \\nAWS CodeCommit Store source code and changes history  \\nAWS CodeBuild Compiles source code and build model  \\nAWS CodePipeline Automate pipeline for CI/CD \\nAmazon CloudWatch Monitor model performance from logs and to send alarm \\nAWS Lambda Serverless computing service to perform inference, update data \\nlabel, and trigger model training pipeline. \\nAPI Gateway Managed s ervice that facilitates interactions between public \\nrequests to AWS services. \\n \\n3.1. Model Building6 \\n3.1.1. Model Building Workflow7 \\nIn the development of the model, the team implemented a continuous integration approach. The \\ncommencement of this process is signaled when the model building code is committed into the repository. This \\nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \\nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \\nare listed as follows: \\n1. The preprocessing of training data. \\n2. The actual training of the model. \\n3. The evaluation of the model. \\n4. The final step, which involves registering the model into the Model Registry.  \\nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \\nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \\nbaseline performance standards for the model. \\n3.1.2. Data Preprocessing \\nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \\nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \\naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \\ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \\nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \\nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \\nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \\n \\n5 See Appendix: Figure E for the final system architecture. \\n6 See Appendix: Figure F for the detailed model building architecture. \\n7 See Appendix: Figure G for the CodePipeline stages for model building.'], reference_answer='The main components of the machine learning system architecture include:\\n\\n1. **Amazon S3 Training Bucket** - Stores training images that will be converted into PyTorch Tensor for model training.\\n2. **Amazon S3 Interim Bucket** - Stores reported/appealed images for moderators to evaluate and take appropriate action.\\n3. **Amazon SageMaker – Training** - Conducts model training, building, and compilation.\\n4. **Amazon SageMaker – Model Registry** - Contains a catalogue of models to track and manage.\\n5. **Amazon SageMaker - Endpoint** - Deploys the model for real-time inference.\\n6. **AWS CodeCommit** - Stores source code and changes history.\\n7. **AWS CodeBuild** - Compiles source code and builds the model.\\n8. **AWS CodePipeline** - Automates the pipeline for Continuous Integration/Continuous Deployment (CI/CD).\\n9. **Amazon CloudWatch** - Monitors model performance from logs and sends alarms.\\n10. **AWS Lambda** - Provides serverless computing services to perform inference, update data labels, and trigger the model training pipeline.\\n11. **API Gateway** - Facilitates interactions between public requests and AWS services.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Amazon S3 Training Bucket:** Stores training images that will be converted into PyTorch Tensor for model training.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['4 \\npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \\ncommunication to our stakeholders on how the data will be used an d protected. \\n3. Machine Learning System Architecture \\nThe team has segmented the pipeline into a few components for the purpose of the project5.  \\nService Explanation \\nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \\nfor model training \\nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \\ntake appropriate action. \\nAmazon SageMaker – Training Conduct model training, building and compilation \\nAmazon SageMaker – Model Registry Contains a catalogue of models to track and manage \\nAmazon SageMaker - Endpoint Deploying model for real-time inference \\nAWS CodeCommit Store source code and changes history  \\nAWS CodeBuild Compiles source code and build model  \\nAWS CodePipeline Automate pipeline for CI/CD \\nAmazon CloudWatch Monitor model performance from logs and to send alarm \\nAWS Lambda Serverless computing service to perform inference, update data \\nlabel, and trigger model training pipeline. \\nAPI Gateway Managed s ervice that facilitates interactions between public \\nrequests to AWS services. \\n \\n3.1. Model Building6 \\n3.1.1. Model Building Workflow7 \\nIn the development of the model, the team implemented a continuous integration approach. The \\ncommencement of this process is signaled when the model building code is committed into the repository. This \\nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \\nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \\nare listed as follows: \\n1. The preprocessing of training data. \\n2. The actual training of the model. \\n3. The evaluation of the model. \\n4. The final step, which involves registering the model into the Model Registry.  \\nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \\nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \\nbaseline performance standards for the model. \\n3.1.2. Data Preprocessing \\nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \\nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \\naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \\ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \\nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \\nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \\nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \\n \\n5 See Appendix: Figure E for the final system architecture. \\n6 See Appendix: Figure F for the detailed model building architecture. \\n7 See Appendix: Figure G for the CodePipeline stages for model building.'], reference_answer='The Amazon S3 Training Bucket is a storage solution used to hold training images that are essential for the model training process. These images will be converted into PyTorch Tensors, which are the data structures used by the PyTorch library for building and training machine learning models. This bucket serves as the initial repository for the images before they undergo preprocessing and are utilized in the training pipeline.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Amazon S3 Interim Bucket:** Stores reported/appealed images for moderators to evaluate and take appropriate action.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['4 \\npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \\ncommunication to our stakeholders on how the data will be used an d protected. \\n3. Machine Learning System Architecture \\nThe team has segmented the pipeline into a few components for the purpose of the project5.  \\nService Explanation \\nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \\nfor model training \\nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \\ntake appropriate action. \\nAmazon SageMaker – Training Conduct model training, building and compilation \\nAmazon SageMaker – Model Registry Contains a catalogue of models to track and manage \\nAmazon SageMaker - Endpoint Deploying model for real-time inference \\nAWS CodeCommit Store source code and changes history  \\nAWS CodeBuild Compiles source code and build model  \\nAWS CodePipeline Automate pipeline for CI/CD \\nAmazon CloudWatch Monitor model performance from logs and to send alarm \\nAWS Lambda Serverless computing service to perform inference, update data \\nlabel, and trigger model training pipeline. \\nAPI Gateway Managed s ervice that facilitates interactions between public \\nrequests to AWS services. \\n \\n3.1. Model Building6 \\n3.1.1. Model Building Workflow7 \\nIn the development of the model, the team implemented a continuous integration approach. The \\ncommencement of this process is signaled when the model building code is committed into the repository. This \\nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \\nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \\nare listed as follows: \\n1. The preprocessing of training data. \\n2. The actual training of the model. \\n3. The evaluation of the model. \\n4. The final step, which involves registering the model into the Model Registry.  \\nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \\nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \\nbaseline performance standards for the model. \\n3.1.2. Data Preprocessing \\nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \\nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \\naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \\ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \\nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \\nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \\nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \\n \\n5 See Appendix: Figure E for the final system architecture. \\n6 See Appendix: Figure F for the detailed model building architecture. \\n7 See Appendix: Figure G for the CodePipeline stages for model building.'], reference_answer='The Amazon S3 Interim Bucket is used to store images that have been reported or appealed. This allows moderators to evaluate these images and take the necessary actions based on their assessments.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Amazon SageMaker – Training:** Conducts model training, building, and compilation.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['4 \\npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \\ncommunication to our stakeholders on how the data will be used an d protected. \\n3. Machine Learning System Architecture \\nThe team has segmented the pipeline into a few components for the purpose of the project5.  \\nService Explanation \\nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \\nfor model training \\nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \\ntake appropriate action. \\nAmazon SageMaker – Training Conduct model training, building and compilation \\nAmazon SageMaker – Model Registry Contains a catalogue of models to track and manage \\nAmazon SageMaker - Endpoint Deploying model for real-time inference \\nAWS CodeCommit Store source code and changes history  \\nAWS CodeBuild Compiles source code and build model  \\nAWS CodePipeline Automate pipeline for CI/CD \\nAmazon CloudWatch Monitor model performance from logs and to send alarm \\nAWS Lambda Serverless computing service to perform inference, update data \\nlabel, and trigger model training pipeline. \\nAPI Gateway Managed s ervice that facilitates interactions between public \\nrequests to AWS services. \\n \\n3.1. Model Building6 \\n3.1.1. Model Building Workflow7 \\nIn the development of the model, the team implemented a continuous integration approach. The \\ncommencement of this process is signaled when the model building code is committed into the repository. This \\nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \\nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \\nare listed as follows: \\n1. The preprocessing of training data. \\n2. The actual training of the model. \\n3. The evaluation of the model. \\n4. The final step, which involves registering the model into the Model Registry.  \\nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \\nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \\nbaseline performance standards for the model. \\n3.1.2. Data Preprocessing \\nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \\nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \\naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \\ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \\nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \\nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \\nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \\n \\n5 See Appendix: Figure E for the final system architecture. \\n6 See Appendix: Figure F for the detailed model building architecture. \\n7 See Appendix: Figure G for the CodePipeline stages for model building.'], reference_answer='Amazon SageMaker – Training is responsible for conducting model training, building, and compilation within the machine learning system architecture. It plays a crucial role in the model development process by facilitating the various stages of training, which include preprocessing training data, training the model, evaluating its performance, and ultimately registering the model into the Model Registry if it meets the required accuracy standards.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Amazon SageMaker – Model Registry:** Contains a catalogue of models to track and manage.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['4 \\npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \\ncommunication to our stakeholders on how the data will be used an d protected. \\n3. Machine Learning System Architecture \\nThe team has segmented the pipeline into a few components for the purpose of the project5.  \\nService Explanation \\nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \\nfor model training \\nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \\ntake appropriate action. \\nAmazon SageMaker – Training Conduct model training, building and compilation \\nAmazon SageMaker – Model Registry Contains a catalogue of models to track and manage \\nAmazon SageMaker - Endpoint Deploying model for real-time inference \\nAWS CodeCommit Store source code and changes history  \\nAWS CodeBuild Compiles source code and build model  \\nAWS CodePipeline Automate pipeline for CI/CD \\nAmazon CloudWatch Monitor model performance from logs and to send alarm \\nAWS Lambda Serverless computing service to perform inference, update data \\nlabel, and trigger model training pipeline. \\nAPI Gateway Managed s ervice that facilitates interactions between public \\nrequests to AWS services. \\n \\n3.1. Model Building6 \\n3.1.1. Model Building Workflow7 \\nIn the development of the model, the team implemented a continuous integration approach. The \\ncommencement of this process is signaled when the model building code is committed into the repository. This \\nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \\nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \\nare listed as follows: \\n1. The preprocessing of training data. \\n2. The actual training of the model. \\n3. The evaluation of the model. \\n4. The final step, which involves registering the model into the Model Registry.  \\nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \\nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \\nbaseline performance standards for the model. \\n3.1.2. Data Preprocessing \\nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \\nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \\naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \\ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \\nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \\nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \\nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \\n \\n5 See Appendix: Figure E for the final system architecture. \\n6 See Appendix: Figure F for the detailed model building architecture. \\n7 See Appendix: Figure G for the CodePipeline stages for model building.'], reference_answer='The Amazon SageMaker Model Registry is a component of the machine learning system architecture that serves as a catalogue for tracking and managing different models. It allows the team to register models after they have been trained and evaluated, ensuring that only models meeting predefined performance standards are stored. This helps in maintaining an organized repository of models, facilitating version control and deployment processes.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Amazon SageMaker - Endpoint:** Deploys the model for real-time inference.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['4 \\npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \\ncommunication to our stakeholders on how the data will be used an d protected. \\n3. Machine Learning System Architecture \\nThe team has segmented the pipeline into a few components for the purpose of the project5.  \\nService Explanation \\nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \\nfor model training \\nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \\ntake appropriate action. \\nAmazon SageMaker – Training Conduct model training, building and compilation \\nAmazon SageMaker – Model Registry Contains a catalogue of models to track and manage \\nAmazon SageMaker - Endpoint Deploying model for real-time inference \\nAWS CodeCommit Store source code and changes history  \\nAWS CodeBuild Compiles source code and build model  \\nAWS CodePipeline Automate pipeline for CI/CD \\nAmazon CloudWatch Monitor model performance from logs and to send alarm \\nAWS Lambda Serverless computing service to perform inference, update data \\nlabel, and trigger model training pipeline. \\nAPI Gateway Managed s ervice that facilitates interactions between public \\nrequests to AWS services. \\n \\n3.1. Model Building6 \\n3.1.1. Model Building Workflow7 \\nIn the development of the model, the team implemented a continuous integration approach. The \\ncommencement of this process is signaled when the model building code is committed into the repository. This \\nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \\nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \\nare listed as follows: \\n1. The preprocessing of training data. \\n2. The actual training of the model. \\n3. The evaluation of the model. \\n4. The final step, which involves registering the model into the Model Registry.  \\nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \\nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \\nbaseline performance standards for the model. \\n3.1.2. Data Preprocessing \\nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \\nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \\naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \\ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \\nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \\nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \\nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \\n \\n5 See Appendix: Figure E for the final system architecture. \\n6 See Appendix: Figure F for the detailed model building architecture. \\n7 See Appendix: Figure G for the CodePipeline stages for model building.'], reference_answer=\"Amazon SageMaker - Endpoint is a service that facilitates the deployment of machine learning models for real-time inference. This allows users to make predictions or obtain insights from the model as soon as it is deployed, enabling immediate application of the model's capabilities in various scenarios.\", reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**AWS CodeCommit:** Stores source code and changes history.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['4 \\npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \\ncommunication to our stakeholders on how the data will be used an d protected. \\n3. Machine Learning System Architecture \\nThe team has segmented the pipeline into a few components for the purpose of the project5.  \\nService Explanation \\nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \\nfor model training \\nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \\ntake appropriate action. \\nAmazon SageMaker – Training Conduct model training, building and compilation \\nAmazon SageMaker – Model Registry Contains a catalogue of models to track and manage \\nAmazon SageMaker - Endpoint Deploying model for real-time inference \\nAWS CodeCommit Store source code and changes history  \\nAWS CodeBuild Compiles source code and build model  \\nAWS CodePipeline Automate pipeline for CI/CD \\nAmazon CloudWatch Monitor model performance from logs and to send alarm \\nAWS Lambda Serverless computing service to perform inference, update data \\nlabel, and trigger model training pipeline. \\nAPI Gateway Managed s ervice that facilitates interactions between public \\nrequests to AWS services. \\n \\n3.1. Model Building6 \\n3.1.1. Model Building Workflow7 \\nIn the development of the model, the team implemented a continuous integration approach. The \\ncommencement of this process is signaled when the model building code is committed into the repository. This \\nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \\nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \\nare listed as follows: \\n1. The preprocessing of training data. \\n2. The actual training of the model. \\n3. The evaluation of the model. \\n4. The final step, which involves registering the model into the Model Registry.  \\nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \\nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \\nbaseline performance standards for the model. \\n3.1.2. Data Preprocessing \\nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \\nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \\naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \\ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \\nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \\nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \\nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \\n \\n5 See Appendix: Figure E for the final system architecture. \\n6 See Appendix: Figure F for the detailed model building architecture. \\n7 See Appendix: Figure G for the CodePipeline stages for model building.'], reference_answer='AWS CodeCommit is a fully managed source control service that allows teams to host secure and scalable Git repositories. It is used to store source code and track changes over time, enabling collaboration among team members. In the context of the machine learning system architecture described, CodeCommit plays a crucial role in maintaining the version history of the model building code, which triggers the continuous integration process when changes are committed. This ensures that the latest code is always used in the model training pipeline.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**AWS CodeBuild:** Compiles source code and builds the model.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['4 \\npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \\ncommunication to our stakeholders on how the data will be used an d protected. \\n3. Machine Learning System Architecture \\nThe team has segmented the pipeline into a few components for the purpose of the project5.  \\nService Explanation \\nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \\nfor model training \\nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \\ntake appropriate action. \\nAmazon SageMaker – Training Conduct model training, building and compilation \\nAmazon SageMaker – Model Registry Contains a catalogue of models to track and manage \\nAmazon SageMaker - Endpoint Deploying model for real-time inference \\nAWS CodeCommit Store source code and changes history  \\nAWS CodeBuild Compiles source code and build model  \\nAWS CodePipeline Automate pipeline for CI/CD \\nAmazon CloudWatch Monitor model performance from logs and to send alarm \\nAWS Lambda Serverless computing service to perform inference, update data \\nlabel, and trigger model training pipeline. \\nAPI Gateway Managed s ervice that facilitates interactions between public \\nrequests to AWS services. \\n \\n3.1. Model Building6 \\n3.1.1. Model Building Workflow7 \\nIn the development of the model, the team implemented a continuous integration approach. The \\ncommencement of this process is signaled when the model building code is committed into the repository. This \\nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \\nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \\nare listed as follows: \\n1. The preprocessing of training data. \\n2. The actual training of the model. \\n3. The evaluation of the model. \\n4. The final step, which involves registering the model into the Model Registry.  \\nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \\nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \\nbaseline performance standards for the model. \\n3.1.2. Data Preprocessing \\nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \\nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \\naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \\ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \\nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \\nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \\nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \\n \\n5 See Appendix: Figure E for the final system architecture. \\n6 See Appendix: Figure F for the detailed model building architecture. \\n7 See Appendix: Figure G for the CodePipeline stages for model building.'], reference_answer='AWS CodeBuild is a fully managed build service that compiles source code, runs tests, and produces software packages that are ready to deploy. In the context of the machine learning system architecture described, CodeBuild is responsible for compiling the source code necessary for building the model. This process is part of the continuous integration approach implemented by the team, ensuring that any changes to the codebase are automatically built and tested, facilitating a smooth development workflow.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**AWS CodePipeline:** Automates the pipeline for Continuous Integration/Continuous Deployment (CI/CD).', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['4 \\npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \\ncommunication to our stakeholders on how the data will be used an d protected. \\n3. Machine Learning System Architecture \\nThe team has segmented the pipeline into a few components for the purpose of the project5.  \\nService Explanation \\nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \\nfor model training \\nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \\ntake appropriate action. \\nAmazon SageMaker – Training Conduct model training, building and compilation \\nAmazon SageMaker – Model Registry Contains a catalogue of models to track and manage \\nAmazon SageMaker - Endpoint Deploying model for real-time inference \\nAWS CodeCommit Store source code and changes history  \\nAWS CodeBuild Compiles source code and build model  \\nAWS CodePipeline Automate pipeline for CI/CD \\nAmazon CloudWatch Monitor model performance from logs and to send alarm \\nAWS Lambda Serverless computing service to perform inference, update data \\nlabel, and trigger model training pipeline. \\nAPI Gateway Managed s ervice that facilitates interactions between public \\nrequests to AWS services. \\n \\n3.1. Model Building6 \\n3.1.1. Model Building Workflow7 \\nIn the development of the model, the team implemented a continuous integration approach. The \\ncommencement of this process is signaled when the model building code is committed into the repository. This \\nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \\nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \\nare listed as follows: \\n1. The preprocessing of training data. \\n2. The actual training of the model. \\n3. The evaluation of the model. \\n4. The final step, which involves registering the model into the Model Registry.  \\nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \\nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \\nbaseline performance standards for the model. \\n3.1.2. Data Preprocessing \\nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \\nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \\naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \\ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \\nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \\nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \\nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \\n \\n5 See Appendix: Figure E for the final system architecture. \\n6 See Appendix: Figure F for the detailed model building architecture. \\n7 See Appendix: Figure G for the CodePipeline stages for model building.'], reference_answer='AWS CodePipeline is a service that automates the pipeline for Continuous Integration/Continuous Deployment (CI/CD). It streamlines the process of building, testing, and deploying applications by integrating various AWS services and tools. In the context of the machine learning system architecture described, CodePipeline initiates the model training pipeline whenever there is a code commit, ensuring that the model is continuously updated and improved based on the latest code changes. This automation helps maintain efficiency and consistency in the development workflow.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Question:** What are the key differences between the ResNet50 and Vision Transformer (ViT-16) models in terms of their architecture and performance based on the context provided?', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['5 \\n3.1.3. Model Training \\nResNet50 \\nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \\nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \\nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \\nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \\nVision Transformer (ViT-16)  \\nVision Transformer is an image clas sification architecture that employs transformer architecture to process \\nimages. It divides the input images into patches and process them through a series of self -attention layers, \\nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \\nlayers, allowing us to capture global dependencies in the image.  \\nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \\nclassifier layer of the following models:  \\nModel Accuracy \\nResnet50 ~20% \\nViT-16 ~60% \\n \\nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \\noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \\nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \\n3.1.4. Model Quantization \\nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \\nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \\ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \\na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \\naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \\nThe un-quantized model size was also relatively manageable at around 300mb.  \\n3.2. Model Deployment8  \\n3.2.1. Model Deployment Workflow9 \\nOur project employs an image classification model designed to operate in a real -time inference setting. Given \\nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \\ndeployment workflow that maximizes both model accuracy and system  reliability. \\nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \\nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \\nsource code or when a new model gets approved in the model registry . The workflow then builds a package \\nfrom the repository, which encompasses both our staging and production deployment CloudFormation \\ntemplate. \\nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \\nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \\ninference test on the staging endpoint. \\nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \\nto automatically deploy the model into production environment. At this stage, the team could conduct \\nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \\nauthorize the deployment into the production environment. \\n \\n8 See Appendix: Figure H for the detailed model deployment architecture. \\n9 See Appendix: Figure I for the CodePipeline stages for model deployment.'], reference_answer=\"The key differences between the ResNet50 and Vision Transformer (ViT-16) models in terms of architecture and performance, based on the provided context, are as follows:\\n\\n### Architecture:\\n1. **ResNet50**:\\n   - Utilizes a deep convolutional neural network architecture that employs residual networks.\\n   - Introduces skip connections to address the vanishing gradient problem, which allows for the training of deeper networks.\\n   - Primarily relies on convolutional layers for feature extraction.\\n\\n2. **Vision Transformer (ViT-16)**:\\n   - Employs a transformer architecture for image classification.\\n   - Divides input images into patches and processes them through a series of self-attention layers, enabling long-range interactions between image elements.\\n   - Eliminates the need for convolutional layers, allowing it to capture global dependencies in the image.\\n\\n### Performance:\\n- **ResNet50**: Achieved an accuracy of approximately 20%.\\n- **ViT-16**: Achieved a significantly higher accuracy of around 60%.\\n\\n### Conclusion:\\nBased on the performance metrics, the ViT-16 model is identified as the more appropriate choice for the task at hand, outperforming ResNet50 by a substantial margin. While both models have their strengths, the ViT-16's architecture allows for better performance in this specific context.\", reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query=\"**Answer:** ResNet50 is a deep convolutional neural network that utilizes residual networks and skip connections to address vanishing gradient problems, making it easier to train deeper networks. In contrast, Vision Transformer (ViT-16) employs a transformer architecture that processes images by dividing them into patches and using self-attention layers, allowing for long-range interactions between image elements and eliminating the need for convolutional layers. In terms of performance, ViT-16 significantly outperforms ResNet50, achieving an accuracy of approximately 60% compared to ResNet50's ~20%.\", query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['5 \\n3.1.3. Model Training \\nResNet50 \\nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \\nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \\nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \\nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \\nVision Transformer (ViT-16)  \\nVision Transformer is an image clas sification architecture that employs transformer architecture to process \\nimages. It divides the input images into patches and process them through a series of self -attention layers, \\nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \\nlayers, allowing us to capture global dependencies in the image.  \\nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \\nclassifier layer of the following models:  \\nModel Accuracy \\nResnet50 ~20% \\nViT-16 ~60% \\n \\nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \\noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \\nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \\n3.1.4. Model Quantization \\nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \\nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \\ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \\na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \\naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \\nThe un-quantized model size was also relatively manageable at around 300mb.  \\n3.2. Model Deployment8  \\n3.2.1. Model Deployment Workflow9 \\nOur project employs an image classification model designed to operate in a real -time inference setting. Given \\nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \\ndeployment workflow that maximizes both model accuracy and system  reliability. \\nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \\nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \\nsource code or when a new model gets approved in the model registry . The workflow then builds a package \\nfrom the repository, which encompasses both our staging and production deployment CloudFormation \\ntemplate. \\nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \\nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \\ninference test on the staging endpoint. \\nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \\nto automatically deploy the model into production environment. At this stage, the team could conduct \\nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \\nauthorize the deployment into the production environment. \\n \\n8 See Appendix: Figure H for the detailed model deployment architecture. \\n9 See Appendix: Figure I for the CodePipeline stages for model deployment.'], reference_answer=\"ResNet50 is a deep convolutional neural network that utilizes residual networks and skip connections to address vanishing gradient problems, making it easier to train deeper networks. In contrast, Vision Transformer (ViT-16) employs a transformer architecture that processes images by dividing them into patches and using self-attention layers, allowing for long-range interactions between image elements and eliminating the need for convolutional layers. In terms of performance, ViT-16 significantly outperforms ResNet50, achieving an accuracy of approximately 60% compared to ResNet50's ~20%.\", reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Question:** What is the purpose of using a Canary deployment strategy in the context of the image classification model described in the report?', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=[\"6 \\nOnce the stage approval is given, which happens after successful testing in the staging environment, the \\nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \\nwithin the CloudFormation template included in the package.  \\n3.2.2. Auto Scaling Policy \\nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \\nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \\nthe “SageMakerVariantInvocationsPerInstance” metric, monitoring a target value of 70 per minute, with a scale \\nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \\naccordingly to fit individual communities as they see fit.  \\n3.2.3. Deployment Strategy \\nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \\nincremental rollout process where new versions of a model (or application) are released to a small, controlled \\nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \\nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \\nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \\noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \\na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \\nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \\nanomalies before it affects all users. \\nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \\na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \\nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \\ndecision about the full deployment of the new model. \\nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \\nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \\nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \\nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \\nperformance consistency of our image classification system. \\nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \\ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \\nstaging and production environment. \\n3.3. Monitoring & Retraining Step \\n3.3.1. User Feedback to handle Concept & Model Drift \\nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \\nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \\ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \\nenvironments, making the model suitable for communities with different social acceptance norms. \\nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \\nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \\nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \\niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \\nreliability and effectiveness of the machine learning model.\"], reference_answer=\"The purpose of using a Canary deployment strategy in the context of the image classification model is to allow for an incremental rollout of new model versions to a small, controlled subset of users or environments before a full deployment. This approach offers several benefits:\\n\\n1. **Performance Monitoring**: It enables the team to monitor the new model's performance, measure prediction accuracy, and identify any unexpected behaviors or anomalies in a live setting without affecting the entire user base.\\n\\n2. **Comparison with Existing Model**: The strategy allows for a direct comparison between the new model version and the existing one under actual load conditions, evaluating metrics such as model latency, throughput, and resource usage.\\n\\n3. **Risk Reduction**: By deploying the new model incrementally, the risks associated with introducing new models are minimized. If issues arise during the Canary phase, the deployment can be quickly rolled back, reducing the impact on the overall system and user experience.\\n\\n4. **Time for Diagnosis**: The Canary deployment provides time to diagnose any issues and make necessary adjustments before a broader rollout, acting as a safety net to ensure high reliability and performance consistency of the image classification system.\\n\\nOverall, the Canary deployment strategy helps maintain a seamless experience for end-users while ensuring the reliability and effectiveness of the model in a dynamic environment.\", reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query=\"**Answer:** The purpose of using a Canary deployment strategy is to release new versions of the model to a small, controlled subset of users or environments before a full rollout. This approach allows the team to test the model's performance, assess potential risks, and detect issues early without affecting the entire user base. It enables monitoring of metrics such as prediction accuracy, model latency, throughput, and resource usage under real-world conditions, thereby reducing the risk associated with deploying new models and providing a safety net for high reliability and performance consistency.\", query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=[\"6 \\nOnce the stage approval is given, which happens after successful testing in the staging environment, the \\nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \\nwithin the CloudFormation template included in the package.  \\n3.2.2. Auto Scaling Policy \\nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \\nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \\nthe “SageMakerVariantInvocationsPerInstance” metric, monitoring a target value of 70 per minute, with a scale \\nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \\naccordingly to fit individual communities as they see fit.  \\n3.2.3. Deployment Strategy \\nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \\nincremental rollout process where new versions of a model (or application) are released to a small, controlled \\nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \\nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \\nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \\noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \\na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \\nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \\nanomalies before it affects all users. \\nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \\na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \\nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \\ndecision about the full deployment of the new model. \\nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \\nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \\nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \\nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \\nperformance consistency of our image classification system. \\nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \\ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \\nstaging and production environment. \\n3.3. Monitoring & Retraining Step \\n3.3.1. User Feedback to handle Concept & Model Drift \\nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \\nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \\ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \\nenvironments, making the model suitable for communities with different social acceptance norms. \\nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \\nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \\nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \\niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \\nreliability and effectiveness of the machine learning model.\"], reference_answer=\"The purpose of using a Canary deployment strategy is to release new versions of the model to a small, controlled subset of users or environments before a full rollout. This approach allows the team to test the model's performance, assess potential risks, and detect issues early without affecting the entire user base. It enables monitoring of metrics such as prediction accuracy, model latency, throughput, and resource usage under real-world conditions. By initially deploying the new model version to a limited subset of traffic, the team can identify any unexpected behaviors or anomalies, compare the new model with the existing one, and quickly rollback the deployment if any problems arise. This incremental rollout reduces the risk associated with deploying new models and provides a safety net for ensuring high reliability and performance consistency of the image classification system.\", reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Question:** How does the implementation of user feedback through the Discord server bot contribute to the model validation process in the image classification project?', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=[\"7 \\n3.3.2. Implementation of User Feedback – Discord Server Bot \\nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \\nwas modified to allow the bot to do the following: \\nContext Bot Actions \\nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \\nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \\nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \\nThese are saved in a local database within the bot along with other metadata (i.e. user, \\nchannel, timestamp) \\nImage result – Safe Does nothing \\nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \\nSends a message in the moderator notification channel with relevant details. Sends a \\nprivate message to the user with reason for the timeout and gives an option to appeal. \\nNSFW user – Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \\nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \\nmoderator notification channel with relevant details. \\nNSFW user – Accept Does nothing \\nUser reports image Sends a message in the moderator notification channel with relevant details. \\n \\nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \\ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \\ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \\ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \\nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \\nthe training bucket.  \\nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \\ntrigger the model training pipeline. \\nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \\nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \\nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \\n3.3.3. AWS CloudWatch Alarms \\nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \\ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \\nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \\ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \\nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \\nto facilitate the monitoring process. \\nWe also set an “Invocation Model Errors” CloudWatch alarm to monitor the number of failed invocations of our \\nimage classification model in the production environment. This alarm is pivotal as it provides immediate \\nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \\nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \\nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \\nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \\nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \\ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \\nreliability of our machine learning system. \\n \\n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \\n11 See Appendix: Figure J for the list of CloudWatch alarms.\"], reference_answer=\"The implementation of user feedback through the Discord server bot contributes to the model validation process in the image classification project in several key ways:\\n\\n1. **User Interaction**: The bot allows users to upload images and receive classification results. This interaction provides real-time feedback on the model's performance, as users can appeal or report incorrect classifications.\\n\\n2. **Error Detection**: When users appeal or report images that are wrongly classified, moderators can review these cases. This process helps identify instances of model drift or inaccuracies in the model's predictions.\\n\\n3. **Data Collection for Training**: Images that are confirmed to be misclassified by moderators are moved into a training bucket to form the ground truth. This ensures that the model is continuously updated with accurate data, improving its future performance.\\n\\n4. **Automated Monitoring**: The system includes a mechanism where images that are not reported are also reviewed and moved to the training bucket every 24 hours. This proactive approach helps maintain the quality of the training data.\\n\\n5. **Threshold-Based Model Retraining**: The implementation includes a threshold for the number of wrongly classified images. When this threshold is crossed, a lambda function triggers the model training pipeline, ensuring that the model is retrained with the latest data reflecting user feedback.\\n\\n6. **Community Engagement**: By involving users in the validation process, the project leverages community insights, which can lead to a more robust understanding of the model's performance and areas for improvement.\\n\\nOverall, the user feedback loop facilitated by the Discord bot enhances the model validation process by ensuring continuous learning and adaptation based on real user experiences and interactions.\", reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query=\"**Answer:** The implementation of user feedback through the Discord server bot contributes to the model validation process by allowing users to interact with the model's predictions. When users upload images, the bot sends the images for classification and stores the results along with metadata. If images are wrongly classified, users can appeal or report them, prompting moderators to verify the labels. This feedback loop helps identify and mitigate drift in the model's performance. Images confirmed to be misclassified are moved to the training bucket to form the ground truth, and when the number of wrongly classified images exceeds a predefined threshold, it triggers the model training pipeline, thereby continuously improving the model's accuracy.\", query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=[\"7 \\n3.3.2. Implementation of User Feedback – Discord Server Bot \\nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \\nwas modified to allow the bot to do the following: \\nContext Bot Actions \\nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \\nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \\nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \\nThese are saved in a local database within the bot along with other metadata (i.e. user, \\nchannel, timestamp) \\nImage result – Safe Does nothing \\nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \\nSends a message in the moderator notification channel with relevant details. Sends a \\nprivate message to the user with reason for the timeout and gives an option to appeal. \\nNSFW user – Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \\nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \\nmoderator notification channel with relevant details. \\nNSFW user – Accept Does nothing \\nUser reports image Sends a message in the moderator notification channel with relevant details. \\n \\nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \\ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \\ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \\ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \\nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \\nthe training bucket.  \\nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \\ntrigger the model training pipeline. \\nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \\nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \\nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \\n3.3.3. AWS CloudWatch Alarms \\nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \\ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \\nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \\ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \\nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \\nto facilitate the monitoring process. \\nWe also set an “Invocation Model Errors” CloudWatch alarm to monitor the number of failed invocations of our \\nimage classification model in the production environment. This alarm is pivotal as it provides immediate \\nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \\nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \\nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \\nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \\nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \\ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \\nreliability of our machine learning system. \\n \\n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \\n11 See Appendix: Figure J for the list of CloudWatch alarms.\"], reference_answer=\"The implementation of user feedback through the Discord server bot enhances the model validation process by facilitating user interaction with the model's predictions. When users upload images, the bot processes these images for classification and records the results along with relevant metadata. In cases of misclassification, users have the option to appeal or report the images, which prompts moderators to verify the accuracy of the labels. This feedback loop is crucial for identifying and addressing model drift, as it allows for the continuous refinement of the model's performance. Images that are confirmed to be misclassified are transferred to the training bucket to establish ground truth data. Additionally, if the number of misclassified images surpasses a specified threshold, it activates the model training pipeline, ensuring ongoing improvements in the model's accuracy.\", reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Question:** What is the significance of monitoring CPU utilization in the context of the image classification model, and what threshold was set for the alarm?', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['8 \\nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \\nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \\nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \\ncomputational load or an up grade to a larger instance type to handle the model\\'s demands better. Despite the \\nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \\ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \\n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \\nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \\nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \\nreal-time explicit image classification service. \\n4. Limitations, Considerations & Future Works \\n4.1. Technical Limitations  \\nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \\nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \\nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \\nsystem transparency and robustness by providing instant insights into the model\\'s decision-making process. This \\ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \\nClarify\\'s SHAP values in real-time. \\nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \\nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \\nbreakdown of how each influences the model\\'s prediction. For our image classification model, SageMaker Clarify \\ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \\nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \\n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \\naccountability to the model\\'s decision-making process. \\n4.2. Data Limitations \\nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \\ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \\nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \\nadapting to the individual communities’ user feedback. \\n4.3. Model Limitations \\nModel Accuracy & Experimentation with Proportion of Class Labels  \\nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \\nthere is considerable room for improvement. One potential avenue for enhancing the model\\'s precision involves \\nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \\nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \\nyield benefits; it allows the model additional opportunities to le arn from the data. However, it\\'s crucial to \\nbalance this with computational resources and the risk of overfitting.  \\nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \\ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \\nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \\nto the platform\\'s reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \\nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \\n \\n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .'], reference_answer=\"Monitoring CPU utilization is significant in the context of the image classification model because it provides valuable insights into the model's computational demands. By keeping an eye on CPU usage, the team can identify potential performance issues and take proactive measures, such as optimizing the model to reduce its computational load or upgrading to a larger instance type to better handle the model's demands. This monitoring helps ensure that there is sufficient computing power to avoid disruptions in service.\\n\\nThe threshold set for the alarm was more than an average of 70% CPU usage over every 5-minute interval based on baseline percentages.\", reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Answer:** Monitoring CPU utilization is significant because it provides valuable insights into the computational demands of the model, allowing for timely optimizations or upgrades to ensure sufficient computing power. The alarm threshold was set to more than an average of 70% CPU usage over every 5-minute interval based on baseline percentages.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['8 \\nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \\nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \\nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \\ncomputational load or an up grade to a larger instance type to handle the model\\'s demands better. Despite the \\nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \\ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \\n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \\nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \\nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \\nreal-time explicit image classification service. \\n4. Limitations, Considerations & Future Works \\n4.1. Technical Limitations  \\nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \\nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \\nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \\nsystem transparency and robustness by providing instant insights into the model\\'s decision-making process. This \\ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \\nClarify\\'s SHAP values in real-time. \\nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \\nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \\nbreakdown of how each influences the model\\'s prediction. For our image classification model, SageMaker Clarify \\ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \\nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \\n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \\naccountability to the model\\'s decision-making process. \\n4.2. Data Limitations \\nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \\ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \\nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \\nadapting to the individual communities’ user feedback. \\n4.3. Model Limitations \\nModel Accuracy & Experimentation with Proportion of Class Labels  \\nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \\nthere is considerable room for improvement. One potential avenue for enhancing the model\\'s precision involves \\nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \\nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \\nyield benefits; it allows the model additional opportunities to le arn from the data. However, it\\'s crucial to \\nbalance this with computational resources and the risk of overfitting.  \\nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \\ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \\nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \\nto the platform\\'s reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \\nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \\n \\n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .'], reference_answer='Monitoring CPU utilization is significant because it provides valuable insights into the computational demands of the model, allowing for timely optimizations or upgrades to ensure sufficient computing power. The alarm threshold was set to more than an average of 70% CPU usage over every 5-minute interval based on baseline percentages. This proactive approach helps in identifying potential disruptions caused by insufficient computing resources, thereby ensuring the provision of a high-quality, real-time explicit image classification service.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Question:** What measures are proposed to monitor and manage Model Prediction Latency in the deployment infrastructure of the project?', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=[\"9 \\nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \\nperformance level in real-world conditions. \\n4.4. Deployment Infrastructure \\nModel Latency \\nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \\nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \\nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \\nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \\non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \\napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \\nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \\npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \\nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \\nunder varying load and user interaction patterns. \\nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \\nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \\nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \\nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \\nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \\nexperience. \\nAdversarial Attacks \\nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \\nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \\nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \\ndeploying “defences” against adversarial attacks can help to mitigate this risk. \\nPipeline Architecture \\nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \\nmay be more justifiable as the use case scales up.  \\n4.5. Ethical & Legal Considerations \\nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \\nconsent. While the images could significantly improve model performance due to their real -world variability, \\nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \\nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \\nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \\nhandling and usage policies, with user consent at the forefront. \\n4.6. Scope Expansion \\nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \\npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \\nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \\nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \\nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \\nthe existing infrastructure.\"], reference_answer=\"To monitor and manage Model Prediction Latency in the deployment infrastructure of the project, the following measures are proposed:\\n\\n1. **Monitoring with CloudWatch**: The project plans to monitor the Model Latency metric under the AWS/SageMaker namespace in CloudWatch. This allows for real-time tracking of latency performance.\\n\\n2. **Setting Acceptable Thresholds**: An acceptable threshold for latency will be defined based on the application's requirements. This helps in establishing a standard for what is considered acceptable latency.\\n\\n3. **CloudWatch Alarms**: Alarms will be set up in CloudWatch to notify the team if the latency exceeds the defined acceptable limit. This proactive approach ensures that any potential issues are addressed promptly.\\n\\n4. **Baseline Establishment**: A suitable baseline for Model Prediction Latency will be established to adequately monitor and react to potential issues in real-time. This baseline will be informed by data collected during a staging test with general users.\\n\\n5. **Data Collection Under Real-World Conditions**: During the staging phase, latency data will be collected under real-world conditions to understand typical latency patterns exhibited by the model under varying loads and user interactions.\\n\\n6. **Trend and Pattern Analysis**: The project will observe and analyze trends and patterns of model latency, considering both average latency and peak times. This analysis will account for user behavior patterns that might impact system load.\\n\\n7. **Setting Realistic Thresholds**: By observing latency patterns, the team aims to set a realistic and acceptable threshold for Model Latency that accommodates typical usage while ensuring swift reactions if latency exceeds expected peaks.\\n\\nThese measures collectively aim to maintain the responsiveness of the service and ensure a seamless user experience.\", reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query=\"**Answer:** The project proposes to monitor Model Prediction Latency under the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based on the application's requirements, CloudWatch alarms will be set up to notify the team if this limit is exceeded. Additionally, during the staging phase with general users, latency data will be collected under real-world conditions to understand typical latency patterns, allowing the team to set a realistic baseline and react swiftly if latency exceeds expected peaks, ensuring a seamless user experience.\", query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=[\"9 \\nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \\nperformance level in real-world conditions. \\n4.4. Deployment Infrastructure \\nModel Latency \\nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \\nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \\nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \\nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \\non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \\napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \\nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \\npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \\nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \\nunder varying load and user interaction patterns. \\nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \\nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \\nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \\nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \\nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \\nexperience. \\nAdversarial Attacks \\nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \\nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \\nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \\ndeploying “defences” against adversarial attacks can help to mitigate this risk. \\nPipeline Architecture \\nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \\nmay be more justifiable as the use case scales up.  \\n4.5. Ethical & Legal Considerations \\nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \\nconsent. While the images could significantly improve model performance due to their real -world variability, \\nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \\nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \\nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \\nhandling and usage policies, with user consent at the forefront. \\n4.6. Scope Expansion \\nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \\npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \\nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \\nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \\nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \\nthe existing infrastructure.\"], reference_answer=\"The project proposes to monitor Model Prediction Latency under the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based on the application's requirements, CloudWatch alarms will be set up to notify the team if this limit is exceeded. Additionally, during the staging phase with general users, latency data will be collected under real-world conditions to understand typical latency patterns. This will enable the team to set a realistic baseline and react swiftly if latency exceeds expected peaks, ensuring a seamless user experience.\", reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Question:** What are some key resources mentioned in the references that discuss content moderation and image classification?', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['10 \\n5. References \\n \\nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \\n https://github.com/alex000kim/nsfw_data_scraper \\nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \\nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\\nclarify/computer_vision/image_classification/explainability_image_classification.html \\nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \\nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\\nfor-online-platforms-how-it-works/ \\nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \\nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\\nimportant-for-online-platforms-how-it-works/ \\nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \\n https://github.com/EBazarov/nsfw_data_source_urls \\nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \\nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \\nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \\nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \\nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \\nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \\nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \\nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \\nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \\nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \\nvideo are shared online daily. can you sort real from fake?. The Conversation. \\nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\\ncan-you-sort-real-from-fake-148630'], reference_answer='Some key resources mentioned in the references that discuss content moderation and image classification include:\\n\\n1. **Amazon Web Services (2020)** - \"Explaining Image Classification with SageMaker Clarify.\" This resource provides insights into image classification using Amazon SageMaker.\\n   - Link: [Amazon SageMaker Examples](https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-clarify/computer_vision/image_classification/explainability_image_classification.html)\\n\\n2. **Brown, R. (2023)** - \"Why social media content moderation is important for online platforms & how it works?\" This article discusses the significance of content moderation in social media.\\n   - Link: [Cogito Blog](https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-for-online-platforms-how-it-works/)\\n\\n3. **Moderating content (2023)** - Amazon Rekognition Developer Guide, which provides information on content moderation using Amazon Rekognition.\\n   - Link: [Amazon Rekognition Developer Guide](https://docs.aws.amazon.com/rekognition/latest/dg/moderation.html)\\n\\n4. **Matheus Oliveira Franca (2021)** - \"Detection and categorization of suggestive thumbnails,\" which addresses the categorization of images in the context of moderation.\\n   - Link: [Diva Portal](https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf)\\n\\nThese resources collectively cover aspects of image classification and the importance of content moderation in online platforms.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Answer:** The references include several key resources on content moderation and image classification, such as:', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['10 \\n5. References \\n \\nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \\n https://github.com/alex000kim/nsfw_data_scraper \\nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \\nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\\nclarify/computer_vision/image_classification/explainability_image_classification.html \\nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \\nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\\nfor-online-platforms-how-it-works/ \\nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \\nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\\nimportant-for-online-platforms-how-it-works/ \\nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \\n https://github.com/EBazarov/nsfw_data_source_urls \\nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \\nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \\nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \\nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \\nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \\nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \\nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \\nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \\nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \\nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \\nvideo are shared online daily. can you sort real from fake?. The Conversation. \\nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\\ncan-you-sort-real-from-fake-148630'], reference_answer='The references include several key resources on content moderation and image classification, such as:\\n\\n1. **Amazon Web Services (2020)** - A guide on explaining image classification using SageMaker Clarify, which provides insights into the workings of image classification models. [Link](https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-clarify/computer_vision/image_classification/explainability_image_classification.html)\\n\\n2. **Cogito Tech LLC (2023)** - An article discussing the importance of social media content moderation and its operational mechanisms. [Link](https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-for-online-platforms-how-it-works/)\\n\\n3. **Moderating content (2023)** - The Amazon Rekognition Developer Guide, which outlines the moderation capabilities of the Rekognition service. [Link](https://docs.aws.amazon.com/rekognition/latest/dg/moderation.html)\\n\\n4. **Kaiming He et al. (2015)** - A foundational paper on deep residual learning for image recognition, which is critical for understanding modern image classification techniques. [Link](https://arxiv.org/abs/1512.03385)\\n\\n5. **Matheus Oliveira Franca (2021)** - A study on the detection and categorization of suggestive thumbnails, relevant for content moderation in visual media. [Link](https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf)\\n\\nThese resources provide valuable insights into the methodologies and technologies used in content moderation and image classification.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='\"Explaining Image Classification with SageMaker Clarify\" from Amazon Web Services, which discusses image classification techniques.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['10 \\n5. References \\n \\nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \\n https://github.com/alex000kim/nsfw_data_scraper \\nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \\nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\\nclarify/computer_vision/image_classification/explainability_image_classification.html \\nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \\nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\\nfor-online-platforms-how-it-works/ \\nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \\nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\\nimportant-for-online-platforms-how-it-works/ \\nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \\n https://github.com/EBazarov/nsfw_data_source_urls \\nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \\nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \\nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \\nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \\nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \\nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \\nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \\nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \\nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \\nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \\nvideo are shared online daily. can you sort real from fake?. The Conversation. \\nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\\ncan-you-sort-real-from-fake-148630'], reference_answer='The reference titled \"Explaining Image Classification with SageMaker Clarify\" from Amazon Web Services discusses techniques related to image classification. It is part of the Amazon SageMaker Examples and can be accessed at the following link: [Explaining Image Classification with SageMaker Clarify](https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-clarify/computer_vision/image_classification/explainability_image_classification.html). This resource provides insights into how image classification works and the tools available for implementing it using SageMaker Clarify.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='\"Why social media content moderation is important for online platforms & how it works?\" by Cogito Tech LLC, which emphasizes the significance of content moderation.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['10 \\n5. References \\n \\nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \\n https://github.com/alex000kim/nsfw_data_scraper \\nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \\nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\\nclarify/computer_vision/image_classification/explainability_image_classification.html \\nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \\nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\\nfor-online-platforms-how-it-works/ \\nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \\nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\\nimportant-for-online-platforms-how-it-works/ \\nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \\n https://github.com/EBazarov/nsfw_data_source_urls \\nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \\nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \\nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \\nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \\nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \\nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \\nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \\nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \\nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \\nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \\nvideo are shared online daily. can you sort real from fake?. The Conversation. \\nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\\ncan-you-sort-real-from-fake-148630'], reference_answer=\"Social media content moderation is crucial for online platforms for several reasons:\\n\\n1. **User Safety**: Content moderation helps protect users from harmful or inappropriate content, such as hate speech, harassment, and explicit material. This creates a safer online environment for all users.\\n\\n2. **Community Standards**: Platforms often have specific guidelines and community standards that dictate what content is acceptable. Moderation ensures that these standards are upheld, fostering a positive community atmosphere.\\n\\n3. **Legal Compliance**: Many countries have laws regulating online content. Effective moderation helps platforms comply with these legal requirements, reducing the risk of legal repercussions.\\n\\n4. **Brand Reputation**: For businesses, maintaining a positive brand image is essential. Content moderation helps prevent the spread of damaging content that could harm a brand's reputation.\\n\\n5. **User Engagement**: A well-moderated platform encourages user engagement by ensuring that discussions remain constructive and relevant, which can enhance user experience and retention.\\n\\n6. **Algorithmic Support**: Content moderation often involves the use of algorithms and AI tools to identify and filter inappropriate content, making the process more efficient and scalable.\\n\\nIn summary, content moderation is vital for maintaining a safe, respectful, and engaging online environment, while also ensuring compliance with legal standards and protecting brand integrity.\", reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='The Amazon Rekognition Developer Guide, which provides information on moderating content using AWS tools.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['10 \\n5. References \\n \\nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \\n https://github.com/alex000kim/nsfw_data_scraper \\nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \\nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\\nclarify/computer_vision/image_classification/explainability_image_classification.html \\nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \\nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\\nfor-online-platforms-how-it-works/ \\nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \\nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\\nimportant-for-online-platforms-how-it-works/ \\nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \\n https://github.com/EBazarov/nsfw_data_source_urls \\nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \\nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \\nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \\nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \\nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \\nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \\nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \\nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \\nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \\nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \\nvideo are shared online daily. can you sort real from fake?. The Conversation. \\nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\\ncan-you-sort-real-from-fake-148630'], reference_answer='The Amazon Rekognition Developer Guide provides information on moderating content using AWS tools. You can access it at the following link: https://docs.aws.amazon.com/rekognition/latest/dg/moderation.html.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='The paper \"Deep Residual Learning for Image Recognition\" by Kaiming He et al., which presents advancements in image recognition technology.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['10 \\n5. References \\n \\nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \\n https://github.com/alex000kim/nsfw_data_scraper \\nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \\nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\\nclarify/computer_vision/image_classification/explainability_image_classification.html \\nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \\nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\\nfor-online-platforms-how-it-works/ \\nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \\nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\\nimportant-for-online-platforms-how-it-works/ \\nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \\n https://github.com/EBazarov/nsfw_data_source_urls \\nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \\nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \\nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \\nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \\nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \\nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \\nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \\nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \\nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \\nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \\nvideo are shared online daily. can you sort real from fake?. The Conversation. \\nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\\ncan-you-sort-real-from-fake-148630'], reference_answer='The paper \"Deep Residual Learning for Image Recognition\" by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun was published on December 10, 2015. It discusses advancements in image recognition technology through the introduction of deep residual networks (ResNets), which address the problem of training very deep neural networks. The paper can be accessed via the arXiv repository at the following link: [arXiv:1512.03385](https://arxiv.org/abs/1512.03385).', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Question:** What types of content are moderated on social media platforms according to the appendix of the report?', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['11 \\n6. Appendix \\n \\nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \\n \\n \\nFigure B: Data Collection Pipeline\\n \\n \\nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)'], reference_answer='The types of content moderated on social media platforms are detailed in Figure A of the appendix in the report. However, the specific types of content are not provided in the context information you shared. To obtain the exact types of content, you would need to refer to Figure A in the appendix of the report.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Answer:** The appendix includes a figure (Figure A) that outlines the types of content moderated on social media platforms, as referenced from Cognito.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['11 \\n6. Appendix \\n \\nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \\n \\n \\nFigure B: Data Collection Pipeline\\n \\n \\nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)'], reference_answer='The appendix includes a figure (Figure A) that outlines the types of content moderated on social media platforms, as referenced from Cognito. Additionally, it contains Figure B, which illustrates the data collection pipeline, and Figure C, which lists the Amazon Rekognition categories as per the Amazon Developer Guide.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Question:** What does Figure D in the eyecatcher project report illustrate?', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['12 \\nFigure D: Dataset Statistics\\n \\n \\nFigure E: Final Overall System Architecture'], reference_answer='Figure D in the eyecatcher project report illustrates dataset statistics.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Answer:** Figure D illustrates the dataset statistics related to the eyecatcher project.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['12 \\nFigure D: Dataset Statistics\\n \\n \\nFigure E: Final Overall System Architecture'], reference_answer='Figure D illustrates the dataset statistics related to the eyecatcher project.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Question:** What are the two figures mentioned in the context that relate to the model building process?', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['13 \\nFigure F: Detailed Architecture for Model Building\\n \\n \\nFigure G: CodePipeline Stages for Model Building'], reference_answer='The two figures mentioned in the context that relate to the model building process are Figure F, which depicts the Detailed Architecture for Model Building, and Figure G, which illustrates the CodePipeline Stages for Model Building.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Answer:** The two figures mentioned are Figure F, which illustrates the Detailed Architecture for Model Building, and Figure G, which outlines the CodePipeline Stages for Model Building.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['13 \\nFigure F: Detailed Architecture for Model Building\\n \\n \\nFigure G: CodePipeline Stages for Model Building'], reference_answer='The two figures mentioned are Figure F, which illustrates the Detailed Architecture for Model Building, and Figure G, which outlines the CodePipeline Stages for Model Building.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Question:** What is depicted in Figure H of the eyecatcher project report?', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['14 \\nFigure H: Detailed Architecture for Model Deployment'], reference_answer='Figure H of the eyecatcher project report depicts the detailed architecture for model deployment.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Answer:** Figure H illustrates the detailed architecture for model deployment.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['14 \\nFigure H: Detailed Architecture for Model Deployment'], reference_answer='Figure H illustrates the detailed architecture for model deployment.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Question:** What is the significance of the CodePipeline stages in the context of model deployment as illustrated in Figure I of the eyecatcher project report?', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['15 \\nFigure I: CodePipeline Stages for Model Deployment'], reference_answer='The significance of the CodePipeline stages in the context of model deployment, as illustrated in Figure I of the eyecatcher project report, lies in their structured approach to automating the deployment process. Each stage in the CodePipeline represents a critical step in the workflow, ensuring that the model is built, tested, and deployed efficiently and reliably. This systematic process helps to minimize errors, streamline updates, and facilitate continuous integration and delivery (CI/CD) of machine learning models. By clearly defining each stage, the CodePipeline enhances collaboration among team members, allows for better tracking of changes, and ultimately leads to faster and more consistent deployment of models into production environments.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Answer:** The CodePipeline stages are crucial for automating the deployment process of machine learning models, ensuring that each step—from code integration to testing and deployment—is systematically executed to maintain efficiency and reliability in the deployment workflow.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['15 \\nFigure I: CodePipeline Stages for Model Deployment'], reference_answer='The CodePipeline stages are essential for automating the deployment process of machine learning models. They ensure that each step—from code integration to testing and deployment—is systematically executed, which helps maintain efficiency and reliability in the deployment workflow.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Question:** What is depicted in Figure K of the provided document?', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['16 \\nFigure J: Cloudwatch Alarms \\n \\nFigure K: SageMaker Clarify Example (from Amazon SageMaker Examples)'], reference_answer='Figure K of the provided document depicts an example of SageMaker Clarify, which is part of Amazon SageMaker Examples.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Answer:** Figure K illustrates an example of SageMaker Clarify, which is part of the Amazon SageMaker Examples.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['16 \\nFigure J: Cloudwatch Alarms \\n \\nFigure K: SageMaker Clarify Example (from Amazon SageMaker Examples)'], reference_answer='Figure K illustrates an example of SageMaker Clarify, which is part of the Amazon SageMaker Examples.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Question:** What is the Word Error Rate (WER) score for the fine-tuned \"wav2vec2-large-960h\" model when evaluated on the cv-valid-test dataset with 6,300 files?', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=[\"Training Report – Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned “wav2vec2-large-960h” ASR model’s inference capabilities against the performance of the pre-trained “wav2vec2-large-960h” baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score “wav2vec2-large-960h” Pre-trained base model  cv-valid-dev 4,076 10.8% “wav2vec2-large-960h” Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% “wav2vec2-large-960h” Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% “wav2vec2-large-960h” Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained “wav2vec2-large-960h” model (without fine-tuning) was approximately 10.8% while WER using fine-tuned “wav2vec2-large-960h” model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset’s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be “accent”. We found that “accent” distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature “accent” and compare our two models (refer to Figure 1 below).\"], reference_answer='The Word Error Rate (WER) score for the fine-tuned \"wav2vec2-large-960h\" model when evaluated on the cv-valid-test dataset with 6,300 files is 7.3%.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Answer:** The WER score for the fine-tuned \"wav2vec2-large-960h\" model when evaluated on the cv-valid-test dataset with 6,300 files is 7.3%.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=[\"Training Report – Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned “wav2vec2-large-960h” ASR model’s inference capabilities against the performance of the pre-trained “wav2vec2-large-960h” baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score “wav2vec2-large-960h” Pre-trained base model  cv-valid-dev 4,076 10.8% “wav2vec2-large-960h” Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% “wav2vec2-large-960h” Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% “wav2vec2-large-960h” Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained “wav2vec2-large-960h” model (without fine-tuning) was approximately 10.8% while WER using fine-tuned “wav2vec2-large-960h” model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset’s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be “accent”. We found that “accent” distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature “accent” and compare our two models (refer to Figure 1 below).\"], reference_answer='The WER score for the fine-tuned \"wav2vec2-large-960h\" model when evaluated on the cv-valid-test dataset with 6,300 files is 7.3%.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Question:** What strategies are proposed to improve the accuracy of the fine-tuned wav2vec2 model for speech recognition across diverse accents?', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['Figure 1: WER scores by “accent” – Baseline test (pre-trained wav2vec2-large-960h) vs Fine-tuned model (6,300 records) From Figure 1, we observed that the baseline model performs well on speech/audio data from regions like the US, Canada and England. The key question now is: How does fine-tuning affect performance across regions? Our fine-tuned model shows improvements in WER scores across most other regions, indicating successful accent mapping. Notably, countries like Singapore and Africa recorded strong improvements while countries like Philippines and India shows less improvements. This could be due to unique speech nuances and pronunciations and more work needs to be done to explore potential solutions. [Part II] Propose series of steps, including datasets and experiments to improve accuracy of fine-tuned wav2vec2 model 1. Dataset Diversification and Augmentations Papers have shown that audio augmentation strategies has led to minor improvements in evaluation scores. In particular (Ko et.al., 2015) demonstrated the benefits of speech perturbations on model performance. Hence, exploring other strategies like speech perturbations, time masking, pitch shift and background noise injection might be beneficial in contributing to a more diverse training dataset, which could be crucial in improvements in model’s generalisability to unique accents like those in India or the Philipines. 2. Integrating External Language Models for enhanced performance. Leveraging Large Language Models (LLMs) for speech recognition is another feasible solution to improve fine-tuning evaluation accuracy. This post-processing strategy (after acoustic model decoding) involves integrating a transformer-based LLM decoder to perform speech recognition as next token prediction (Hono et.al., 2023). In the context of HuggingFace processors, we can implement one with a decoder that includes an Language Model such as “Facebook/wav2vec2-large-960h-lv60-self”. As it was observed that there were some spelling mistakes contributing to error percentages in WER, these context-aware corrections and output re-ranking strategy could potentially improve WER accuracy in speech transcriptions after model fine-tuning. 3. Hyperparameter Tuning and Fine-tune Model over entire “cv-valid-train” Dataset (195,776 records) Our current approach used open-source past projects as reference points for hyperparameter settings. Some sources include HuggingFace articles (with example colab notebooks), Medium and “readthedocs” articles. In future experiments, we could incorporate methodologies such as random search or Bayesian optimisation to determine optimal hyperparameters for fine-tuning our wav2vec2 model. Another key limitation of this project is compute and memory limitations. We were only able to fine-tune our pre-trained  “wav2vec2-large-960h” model on 6,300 audio files. Therefore, if resources permit, utilizing a large dataset for fine-tuning, coupled with hyperparameter tuning'], reference_answer=\"The proposed strategies to improve the accuracy of the fine-tuned wav2vec2 model for speech recognition across diverse accents include:\\n\\n1. **Dataset Diversification and Augmentations**: Implementing audio augmentation strategies such as speech perturbations, time masking, pitch shifting, and background noise injection to create a more diverse training dataset. This aims to enhance the model's generalizability to unique accents, particularly those in regions like India and the Philippines.\\n\\n2. **Integrating External Language Models**: Leveraging Large Language Models (LLMs) for speech recognition as a post-processing strategy. This involves integrating a transformer-based LLM decoder to perform speech recognition through next token prediction, which can help correct spelling mistakes and improve WER accuracy in speech transcriptions.\\n\\n3. **Hyperparameter Tuning and Fine-tuning on a Larger Dataset**: Conducting hyperparameter tuning using methodologies like random search or Bayesian optimization to find optimal settings for fine-tuning the wav2vec2 model. Additionally, if resources allow, fine-tuning the model on a larger dataset (195,776 records) instead of the current 6,300 audio files to enhance performance.\", reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Answer:** The proposed strategies include:', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['Figure 1: WER scores by “accent” – Baseline test (pre-trained wav2vec2-large-960h) vs Fine-tuned model (6,300 records) From Figure 1, we observed that the baseline model performs well on speech/audio data from regions like the US, Canada and England. The key question now is: How does fine-tuning affect performance across regions? Our fine-tuned model shows improvements in WER scores across most other regions, indicating successful accent mapping. Notably, countries like Singapore and Africa recorded strong improvements while countries like Philippines and India shows less improvements. This could be due to unique speech nuances and pronunciations and more work needs to be done to explore potential solutions. [Part II] Propose series of steps, including datasets and experiments to improve accuracy of fine-tuned wav2vec2 model 1. Dataset Diversification and Augmentations Papers have shown that audio augmentation strategies has led to minor improvements in evaluation scores. In particular (Ko et.al., 2015) demonstrated the benefits of speech perturbations on model performance. Hence, exploring other strategies like speech perturbations, time masking, pitch shift and background noise injection might be beneficial in contributing to a more diverse training dataset, which could be crucial in improvements in model’s generalisability to unique accents like those in India or the Philipines. 2. Integrating External Language Models for enhanced performance. Leveraging Large Language Models (LLMs) for speech recognition is another feasible solution to improve fine-tuning evaluation accuracy. This post-processing strategy (after acoustic model decoding) involves integrating a transformer-based LLM decoder to perform speech recognition as next token prediction (Hono et.al., 2023). In the context of HuggingFace processors, we can implement one with a decoder that includes an Language Model such as “Facebook/wav2vec2-large-960h-lv60-self”. As it was observed that there were some spelling mistakes contributing to error percentages in WER, these context-aware corrections and output re-ranking strategy could potentially improve WER accuracy in speech transcriptions after model fine-tuning. 3. Hyperparameter Tuning and Fine-tune Model over entire “cv-valid-train” Dataset (195,776 records) Our current approach used open-source past projects as reference points for hyperparameter settings. Some sources include HuggingFace articles (with example colab notebooks), Medium and “readthedocs” articles. In future experiments, we could incorporate methodologies such as random search or Bayesian optimisation to determine optimal hyperparameters for fine-tuning our wav2vec2 model. Another key limitation of this project is compute and memory limitations. We were only able to fine-tune our pre-trained  “wav2vec2-large-960h” model on 6,300 audio files. Therefore, if resources permit, utilizing a large dataset for fine-tuning, coupled with hyperparameter tuning'], reference_answer=\"The proposed strategies include:\\n\\n1. **Dataset Diversification and Augmentations**: Implementing audio augmentation strategies such as speech perturbations, time masking, pitch shifting, and background noise injection to create a more diverse training dataset, which could enhance the model's generalizability to unique accents.\\n\\n2. **Integrating External Language Models for Enhanced Performance**: Leveraging Large Language Models (LLMs) for speech recognition by integrating a transformer-based LLM decoder to perform speech recognition as next token prediction. This could help correct spelling mistakes and improve WER accuracy in speech transcriptions.\\n\\n3. **Hyperparameter Tuning and Fine-tuning on a Larger Dataset**: Utilizing methodologies like random search or Bayesian optimization to determine optimal hyperparameters for fine-tuning the wav2vec2 model. Additionally, if resources allow, fine-tuning the model on a larger dataset (195,776 records) to improve performance.\", reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Dataset Diversification and Augmentations**: Implementing audio augmentation techniques such as speech perturbations, time masking, pitch shifting, and background noise injection to create a more diverse training dataset.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['Figure 1: WER scores by “accent” – Baseline test (pre-trained wav2vec2-large-960h) vs Fine-tuned model (6,300 records) From Figure 1, we observed that the baseline model performs well on speech/audio data from regions like the US, Canada and England. The key question now is: How does fine-tuning affect performance across regions? Our fine-tuned model shows improvements in WER scores across most other regions, indicating successful accent mapping. Notably, countries like Singapore and Africa recorded strong improvements while countries like Philippines and India shows less improvements. This could be due to unique speech nuances and pronunciations and more work needs to be done to explore potential solutions. [Part II] Propose series of steps, including datasets and experiments to improve accuracy of fine-tuned wav2vec2 model 1. Dataset Diversification and Augmentations Papers have shown that audio augmentation strategies has led to minor improvements in evaluation scores. In particular (Ko et.al., 2015) demonstrated the benefits of speech perturbations on model performance. Hence, exploring other strategies like speech perturbations, time masking, pitch shift and background noise injection might be beneficial in contributing to a more diverse training dataset, which could be crucial in improvements in model’s generalisability to unique accents like those in India or the Philipines. 2. Integrating External Language Models for enhanced performance. Leveraging Large Language Models (LLMs) for speech recognition is another feasible solution to improve fine-tuning evaluation accuracy. This post-processing strategy (after acoustic model decoding) involves integrating a transformer-based LLM decoder to perform speech recognition as next token prediction (Hono et.al., 2023). In the context of HuggingFace processors, we can implement one with a decoder that includes an Language Model such as “Facebook/wav2vec2-large-960h-lv60-self”. As it was observed that there were some spelling mistakes contributing to error percentages in WER, these context-aware corrections and output re-ranking strategy could potentially improve WER accuracy in speech transcriptions after model fine-tuning. 3. Hyperparameter Tuning and Fine-tune Model over entire “cv-valid-train” Dataset (195,776 records) Our current approach used open-source past projects as reference points for hyperparameter settings. Some sources include HuggingFace articles (with example colab notebooks), Medium and “readthedocs” articles. In future experiments, we could incorporate methodologies such as random search or Bayesian optimisation to determine optimal hyperparameters for fine-tuning our wav2vec2 model. Another key limitation of this project is compute and memory limitations. We were only able to fine-tune our pre-trained  “wav2vec2-large-960h” model on 6,300 audio files. Therefore, if resources permit, utilizing a large dataset for fine-tuning, coupled with hyperparameter tuning'], reference_answer=\"To implement audio augmentation techniques for dataset diversification, you can follow these steps:\\n\\n1. **Speech Perturbations**: Introduce variations in the audio recordings by applying small changes to the pitch, speed, or volume. This can help the model learn to recognize speech under different conditions. For instance, you can use libraries like `librosa` to manipulate audio files by altering their pitch or tempo.\\n\\n2. **Time Masking**: Randomly mask sections of the audio signal to simulate missing data. This technique encourages the model to focus on the context of the remaining audio. You can implement time masking by selecting random segments of the audio and replacing them with silence or noise.\\n\\n3. **Pitch Shifting**: Change the pitch of the audio recordings without altering the speed. This can be done using audio processing libraries such as `pydub` or `librosa`. By shifting the pitch up or down, you can create variations that help the model generalize better across different accents.\\n\\n4. **Background Noise Injection**: Add various types of background noise (e.g., white noise, street sounds, chatter) to the audio recordings. This simulates real-world conditions where speech is often accompanied by background sounds. You can mix the original audio with noise at different levels to create a range of noisy samples.\\n\\n5. **Combining Techniques**: Use a combination of the above techniques to create a more diverse dataset. For example, you could apply time masking followed by pitch shifting and then inject background noise. This will result in a richer set of training examples that can improve the model's robustness.\\n\\n6. **Evaluation of Augmented Data**: After augmenting the dataset, it's essential to evaluate the impact of these techniques on model performance. You can compare the WER scores of models trained on the original dataset versus those trained on the augmented dataset to assess improvements.\\n\\n7. **Iterative Process**: Continuously refine your augmentation strategies based on the model's performance and feedback. Experiment with different combinations and intensities of augmentations to find the optimal setup for your specific use case.\\n\\nBy implementing these audio augmentation techniques, you can create a more diverse training dataset that enhances the model's ability to generalize across various accents and speech patterns.\", reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Integrating External Language Models**: Leveraging Large Language Models (LLMs) for post-processing to enhance speech recognition accuracy by correcting context-aware errors and re-ranking outputs.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['Figure 1: WER scores by “accent” – Baseline test (pre-trained wav2vec2-large-960h) vs Fine-tuned model (6,300 records) From Figure 1, we observed that the baseline model performs well on speech/audio data from regions like the US, Canada and England. The key question now is: How does fine-tuning affect performance across regions? Our fine-tuned model shows improvements in WER scores across most other regions, indicating successful accent mapping. Notably, countries like Singapore and Africa recorded strong improvements while countries like Philippines and India shows less improvements. This could be due to unique speech nuances and pronunciations and more work needs to be done to explore potential solutions. [Part II] Propose series of steps, including datasets and experiments to improve accuracy of fine-tuned wav2vec2 model 1. Dataset Diversification and Augmentations Papers have shown that audio augmentation strategies has led to minor improvements in evaluation scores. In particular (Ko et.al., 2015) demonstrated the benefits of speech perturbations on model performance. Hence, exploring other strategies like speech perturbations, time masking, pitch shift and background noise injection might be beneficial in contributing to a more diverse training dataset, which could be crucial in improvements in model’s generalisability to unique accents like those in India or the Philipines. 2. Integrating External Language Models for enhanced performance. Leveraging Large Language Models (LLMs) for speech recognition is another feasible solution to improve fine-tuning evaluation accuracy. This post-processing strategy (after acoustic model decoding) involves integrating a transformer-based LLM decoder to perform speech recognition as next token prediction (Hono et.al., 2023). In the context of HuggingFace processors, we can implement one with a decoder that includes an Language Model such as “Facebook/wav2vec2-large-960h-lv60-self”. As it was observed that there were some spelling mistakes contributing to error percentages in WER, these context-aware corrections and output re-ranking strategy could potentially improve WER accuracy in speech transcriptions after model fine-tuning. 3. Hyperparameter Tuning and Fine-tune Model over entire “cv-valid-train” Dataset (195,776 records) Our current approach used open-source past projects as reference points for hyperparameter settings. Some sources include HuggingFace articles (with example colab notebooks), Medium and “readthedocs” articles. In future experiments, we could incorporate methodologies such as random search or Bayesian optimisation to determine optimal hyperparameters for fine-tuning our wav2vec2 model. Another key limitation of this project is compute and memory limitations. We were only able to fine-tune our pre-trained  “wav2vec2-large-960h” model on 6,300 audio files. Therefore, if resources permit, utilizing a large dataset for fine-tuning, coupled with hyperparameter tuning'], reference_answer=\"Integrating External Language Models (LLMs) for post-processing in speech recognition involves using these models to improve the accuracy of transcriptions generated by acoustic models like wav2vec2. This approach focuses on correcting context-aware errors and re-ranking outputs to enhance overall performance.\\n\\n1. **Post-Processing Strategy**: After the initial decoding of audio into text by the acoustic model, an LLM can be employed to refine the output. This involves treating the transcription task as a next-token prediction problem, where the LLM predicts the most likely sequence of words based on the context provided by the preceding text.\\n\\n2. **Implementation**: In practical terms, this can be achieved using frameworks like HuggingFace, where a transformer-based LLM decoder can be integrated with the wav2vec2 model. For instance, using a model like “Facebook/wav2vec2-large-960h-lv60-self” can help in generating more accurate transcriptions by leveraging the language model's understanding of syntax and semantics.\\n\\n3. **Error Correction**: One of the key benefits of using LLMs is their ability to identify and correct spelling mistakes or contextually inappropriate words that may have been misrecognized by the acoustic model. This is particularly useful in scenarios where certain accents or pronunciations lead to higher error rates in transcription.\\n\\n4. **Output Re-Ranking**: The LLM can also be used to re-rank multiple transcription hypotheses generated by the acoustic model. By evaluating the likelihood of each hypothesis based on the context, the LLM can help select the most probable and coherent transcription, thereby reducing the Word Error Rate (WER).\\n\\n5. **Potential Benefits**: This integration can lead to significant improvements in WER accuracy, especially in diverse linguistic contexts where unique speech patterns may challenge traditional acoustic models. By incorporating context-aware corrections, the overall robustness and reliability of speech recognition systems can be enhanced.\\n\\nIn summary, leveraging LLMs for post-processing in speech recognition can effectively address context-aware errors and improve transcription accuracy through intelligent correction and output re-ranking strategies.\", reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Hyperparameter Tuning**: Utilizing methodologies like random search or Bayesian optimization to find optimal hyperparameters for fine-tuning the model over a larger dataset (195,776 records) to improve performance.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['Figure 1: WER scores by “accent” – Baseline test (pre-trained wav2vec2-large-960h) vs Fine-tuned model (6,300 records) From Figure 1, we observed that the baseline model performs well on speech/audio data from regions like the US, Canada and England. The key question now is: How does fine-tuning affect performance across regions? Our fine-tuned model shows improvements in WER scores across most other regions, indicating successful accent mapping. Notably, countries like Singapore and Africa recorded strong improvements while countries like Philippines and India shows less improvements. This could be due to unique speech nuances and pronunciations and more work needs to be done to explore potential solutions. [Part II] Propose series of steps, including datasets and experiments to improve accuracy of fine-tuned wav2vec2 model 1. Dataset Diversification and Augmentations Papers have shown that audio augmentation strategies has led to minor improvements in evaluation scores. In particular (Ko et.al., 2015) demonstrated the benefits of speech perturbations on model performance. Hence, exploring other strategies like speech perturbations, time masking, pitch shift and background noise injection might be beneficial in contributing to a more diverse training dataset, which could be crucial in improvements in model’s generalisability to unique accents like those in India or the Philipines. 2. Integrating External Language Models for enhanced performance. Leveraging Large Language Models (LLMs) for speech recognition is another feasible solution to improve fine-tuning evaluation accuracy. This post-processing strategy (after acoustic model decoding) involves integrating a transformer-based LLM decoder to perform speech recognition as next token prediction (Hono et.al., 2023). In the context of HuggingFace processors, we can implement one with a decoder that includes an Language Model such as “Facebook/wav2vec2-large-960h-lv60-self”. As it was observed that there were some spelling mistakes contributing to error percentages in WER, these context-aware corrections and output re-ranking strategy could potentially improve WER accuracy in speech transcriptions after model fine-tuning. 3. Hyperparameter Tuning and Fine-tune Model over entire “cv-valid-train” Dataset (195,776 records) Our current approach used open-source past projects as reference points for hyperparameter settings. Some sources include HuggingFace articles (with example colab notebooks), Medium and “readthedocs” articles. In future experiments, we could incorporate methodologies such as random search or Bayesian optimisation to determine optimal hyperparameters for fine-tuning our wav2vec2 model. Another key limitation of this project is compute and memory limitations. We were only able to fine-tune our pre-trained  “wav2vec2-large-960h” model on 6,300 audio files. Therefore, if resources permit, utilizing a large dataset for fine-tuning, coupled with hyperparameter tuning'], reference_answer='To improve the performance of the fine-tuned wav2vec2 model through hyperparameter tuning, we can implement the following steps:\\n\\n1. **Define Hyperparameter Space**: Identify the hyperparameters that significantly impact model performance. Common hyperparameters for models like wav2vec2 include learning rate, batch size, number of training epochs, dropout rate, and weight decay.\\n\\n2. **Select Tuning Methodology**:\\n   - **Random Search**: This method involves randomly sampling from the defined hyperparameter space. It is simple to implement and can be effective in finding good hyperparameters without exhaustive searching.\\n   - **Bayesian Optimization**: This approach uses a probabilistic model to predict the performance of hyperparameters and iteratively selects the next set of hyperparameters to evaluate based on past results. It is generally more efficient than random search, especially in high-dimensional spaces.\\n\\n3. **Set Up Evaluation Metrics**: Determine the evaluation metrics to assess model performance, such as Word Error Rate (WER) or accuracy on a validation set. This will guide the tuning process.\\n\\n4. **Implement Cross-Validation**: Utilize k-fold cross-validation on the training dataset (195,776 records) to ensure that the hyperparameter tuning is robust and not overfitting to a specific subset of the data.\\n\\n5. **Run Experiments**: Execute the hyperparameter tuning process using the selected methodology. For random search, run multiple trials with different random combinations of hyperparameters. For Bayesian optimization, iteratively refine the search based on previous results.\\n\\n6. **Analyze Results**: After completing the tuning process, analyze the results to identify the best-performing hyperparameters. Compare the performance of the model with these optimal hyperparameters against the baseline and previously fine-tuned models.\\n\\n7. **Fine-tune the Model**: Once optimal hyperparameters are identified, fine-tune the wav2vec2 model on the entire dataset (195,776 records) using these settings to maximize performance.\\n\\n8. **Monitor Resource Usage**: Given the compute and memory limitations mentioned, monitor resource usage during the tuning process to ensure that the experiments are feasible within the available infrastructure.\\n\\nBy following these steps, we can effectively utilize hyperparameter tuning methodologies like random search or Bayesian optimization to enhance the performance of the fine-tuned wav2vec2 model on a larger dataset.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Question:** What strategies are recommended to enhance the training data quality for model fine-tuning in speech recognition, as discussed in the provided context?', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['to optimize model training might improve overall evaluation performance of the pre-trained model, leading to more accurate inferencing results. 4. Exploration of Other Methodologies to Enhance Training Data Quality for Model Fine-tuning Conventional strategies like dataset augmentation and the integration of external language models have been shown to improve model fine-tuning performance in WER scores. Inspired by Guo et. al., 2024, we recommend experimenting with a semi-supervised learning strategy where we utilise self-transcribed, high confidence data to supplement the training data pool for model fine-tuning. These transcribed data can be selected based on model confidence levels (eg. WER <= 0.3).  [Part III] Conclusion The fine-tuning of the wav2vec2-large-960h model on the Common V oice dataset resulted in a notable WER improvement over the baseline model, demonstrating the benefits of domain adaptation. Specifically, fine-tuning allowed the model to better align with accent variations and speech patterns, leading to improved transcription accuracy across diverse regions. However, performance discrepancies across certain accents indicate areas for further refinement. To further enhance inferencing accuracy, we propose a multi-faceted approach involving dataset diversification, augmentation techniques, integration of external language models, and hyperparameter tuning. Additionally, semi-supervised learning strategies could leverage high-confidence transcriptions to expand training data, reducing WER even further. By implementing these enhancements, we aim to develop a more robust and generalizable ASR model, capable of accurately transcribing speech across diverse linguistic and acoustic conditions. References Ko, T., Peddinti, V ., Povey, D., & Khudanpur, S. (2015). Audio augmentation for speech recognition. Interspeech 2015. Retrieved from https://www.isca-archive.org/interspeech_2015/ko15_interspeech.pdf  Hono, S., Kanda, N., Yoshioka, T., Wu, C., Li, X., & Xiao, X. (2023). Transformer-based language models for speech recognition post-processing. arXiv preprint arXiv:2312.03668. Retrieved from https://arxiv.org/pdf/2312.03668  Guo, J., Liu, Z., Zhang, T., & Chen, C. L. P. (2024). Incremental self-training for semi-supervised learning. arXiv preprint arXiv:2404.12398. Retrieved from https://arxiv.org/abs/2404.12398'], reference_answer='The recommended strategies to enhance the training data quality for model fine-tuning in speech recognition include:\\n\\n1. **Dataset Augmentation**: Utilizing conventional strategies to augment the dataset can improve model fine-tuning performance.\\n\\n2. **Integration of External Language Models**: Incorporating external language models has been shown to enhance performance in word error rate (WER) scores.\\n\\n3. **Semi-Supervised Learning Strategy**: Experimenting with a semi-supervised learning approach that utilizes self-transcribed, high-confidence data to supplement the training data pool. This involves selecting transcribed data based on model confidence levels (e.g., WER <= 0.3).\\n\\n4. **Dataset Diversification**: Expanding the diversity of the training dataset to better capture various speech patterns and accents.\\n\\n5. **Hyperparameter Tuning**: Adjusting hyperparameters to optimize model performance.\\n\\nBy implementing these strategies, the goal is to develop a more robust and generalizable automatic speech recognition (ASR) model.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>)), LabelledRagDataExample(query='**Answer:** The recommended strategies to enhance training data quality for model fine-tuning include dataset diversification, augmentation techniques, integration of external language models, hyperparameter tuning, and the use of semi-supervised learning strategies that leverage self-transcribed, high-confidence data to supplement the training data pool.', query_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>), reference_contexts=['to optimize model training might improve overall evaluation performance of the pre-trained model, leading to more accurate inferencing results. 4. Exploration of Other Methodologies to Enhance Training Data Quality for Model Fine-tuning Conventional strategies like dataset augmentation and the integration of external language models have been shown to improve model fine-tuning performance in WER scores. Inspired by Guo et. al., 2024, we recommend experimenting with a semi-supervised learning strategy where we utilise self-transcribed, high confidence data to supplement the training data pool for model fine-tuning. These transcribed data can be selected based on model confidence levels (eg. WER <= 0.3).  [Part III] Conclusion The fine-tuning of the wav2vec2-large-960h model on the Common V oice dataset resulted in a notable WER improvement over the baseline model, demonstrating the benefits of domain adaptation. Specifically, fine-tuning allowed the model to better align with accent variations and speech patterns, leading to improved transcription accuracy across diverse regions. However, performance discrepancies across certain accents indicate areas for further refinement. To further enhance inferencing accuracy, we propose a multi-faceted approach involving dataset diversification, augmentation techniques, integration of external language models, and hyperparameter tuning. Additionally, semi-supervised learning strategies could leverage high-confidence transcriptions to expand training data, reducing WER even further. By implementing these enhancements, we aim to develop a more robust and generalizable ASR model, capable of accurately transcribing speech across diverse linguistic and acoustic conditions. References Ko, T., Peddinti, V ., Povey, D., & Khudanpur, S. (2015). Audio augmentation for speech recognition. Interspeech 2015. Retrieved from https://www.isca-archive.org/interspeech_2015/ko15_interspeech.pdf  Hono, S., Kanda, N., Yoshioka, T., Wu, C., Li, X., & Xiao, X. (2023). Transformer-based language models for speech recognition post-processing. arXiv preprint arXiv:2312.03668. Retrieved from https://arxiv.org/pdf/2312.03668  Guo, J., Liu, Z., Zhang, T., & Chen, C. L. P. (2024). Incremental self-training for semi-supervised learning. arXiv preprint arXiv:2404.12398. Retrieved from https://arxiv.org/abs/2404.12398'], reference_answer='The recommended strategies to enhance training data quality for model fine-tuning include dataset diversification, augmentation techniques, integration of external language models, hyperparameter tuning, and the use of semi-supervised learning strategies that leverage self-transcribed, high-confidence data to supplement the training data pool.', reference_answer_by=CreatedBy(model_name='gpt-4o-mini', type=<CreatedByType.AI: 'ai'>))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7d9dd1",
   "metadata": {},
   "source": [
    "*Save evaluation dataset for future evaluations after implementations*\n",
    "\n",
    "***Adapted from:** https://github.com/tituslhy/ideal-palm-tree/blob/main/notebooks/1a.%20training_dataset_gen.ipynb*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f992b489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Build a list of flat dicts, serializing each field properly\n",
    "records = []\n",
    "for ex in rag_dataset.examples:\n",
    "    records.append({\n",
    "        \"query\": ex.query,\n",
    "        # JSON-encode the list of contexts\n",
    "        \"reference_contexts\": json.dumps(ex.reference_contexts),\n",
    "        \"reference_answer\": ex.reference_answer,\n",
    "        # JSON-encode the CreatedBy objects\n",
    "        \"query_by\": ex.query_by.model_dump_json(),\n",
    "        \"reference_answer_by\": ex.reference_answer_by.model_dump_json(),\n",
    "    })\n",
    "\n",
    "# 2. Turn into a DataFrame and write to CSV\n",
    "df = pd.DataFrame.from_records(records)\n",
    "df.to_csv(\"data/eval_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3103bfa6",
   "metadata": {},
   "source": [
    "*Retrieve evaluation dataset from csv file.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e49cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset import (\n",
    "    LabelledRagDataset,\n",
    "    LabelledRagDataExample,\n",
    "    CreatedBy,\n",
    ")\n",
    "\n",
    "\n",
    "def get_rag_dataset_from_csv(csv_path: str):\n",
    "    converters = {\n",
    "        \"reference_contexts\":    lambda s: json.loads(s),\n",
    "        \"query_by\":             lambda s: CreatedBy.model_validate_json(s),\n",
    "        \"reference_answer_by\":  lambda s: CreatedBy.model_validate_json(s),\n",
    "    }\n",
    "    df = pd.read_csv(csv_path, converters=converters)\n",
    "    examples = []\n",
    "    for _, row in df.iterrows():\n",
    "        examples.append(\n",
    "            LabelledRagDataExample(\n",
    "                query=row[\"query\"],\n",
    "                query_by=row[\"query_by\"],                      # now a CreatedBy\n",
    "                reference_contexts=row[\"reference_contexts\"],   # now a List[str]\n",
    "                reference_answer=row[\"reference_answer\"],\n",
    "                reference_answer_by=row[\"reference_answer_by\"], # now a CreatedBy\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset = LabelledRagDataset(examples=examples)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea80510d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset = get_rag_dataset_from_csv(\"data/eval_dataset.csv\")\n",
    "len(eval_dataset.examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2833d211",
   "metadata": {},
   "source": [
    "### Build Query Engine (RAG System) and Run Baseline Evaluation\n",
    "*With our evaluation dataset in place, we first run evaluation on \"Research Assistant v2\", our baseline RAG model with custom embeddings model and GPT-4o-mini as both our answer generator and judge/evaluator LLM.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a6735ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Instantiate query engine LLM\n",
    "llm = Ollama(model=\"llama3.2:1b\", request_timeout=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77e2cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 1/1 [00:07<00:00,  7.41s/it]\n"
     ]
    }
   ],
   "source": [
    "# Input documents (in index), embedding model and LLM to generate query engine (RAG system)\n",
    "docs = SimpleDirectoryReader(\"../RAG-webscraper/docs/\").load_data(show_progress=True) # load documents from pdf format\n",
    "index = VectorStoreIndex.from_documents(docs, embed_model=Settings.embed_model)\n",
    "query_engine = index.as_query_engine(similarity_top_k=6, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc9e3977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context information, it appears that Snape supported Harry because of Dumbledore's words and the understanding that Snape's true allegiance lies with the Order of the Phoenix. \n",
      "\n",
      "In Chapter 33, Snape is told by Dumbledore to suggest to the Order of the Phoenix that they use decoys, specifically Polyjuice Potion, and identical Potters. This means that Snape deliberately chose to support Harry despite being a Death Eater, as it directly benefits the greater good (Harry's safety) rather than advancing Voldemort's interests.\n",
      "\n",
      "As for hiding his allegiance with the Order from Voldemort, Snape appears to be taking the opportunity to play on Dumbledore's trust and use it against him. He does this by feigning agreement with Dumbledore's request for a warning message about Voldemort's prophecy of Harry being born at the end of July, which he claims was merely an accident. This allows Snape to maintain his cover as a loyal Death Eater while also subtly playing on Dumbledore's trust and exploiting it to further Harry's safety.\n"
     ]
    }
   ],
   "source": [
    "# Test out query engine\n",
    "response = query_engine.query(\"How did Snape support Harry despite being a deatheater? On top of that, how did he hide his allegiance with the order from Voldermort?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade33a75",
   "metadata": {},
   "source": [
    "**Note:** In this scenario, though GPT-4o-mini is a great LLM, we should NOT utilize the same LLM as judge/evaluator and answer generator (in RAG system). Hence, with reference with our earlier demonstration of \"Research Assistant v2\", we kept \"llama3.2:1b\" as the answer generator LLM in our query engine. \n",
    "\n",
    "*Might be useful to consider utilizing a more powerful LLM (GPT-4o-mini) to judge/evaluate the answer generator LLM to derive a more \"precise/critical\" benchmark.* - *Refer to https://github.com/tituslhy/ideal-palm-tree/blob/main/notebooks/2.%20llama32_1bn_RAFT.ipynb*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e66226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-evaluation/pack\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: pandas in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-packs-rag-evaluator==0.3.1) (2.2.3)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.4,>=0.3.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-packs-rag-evaluator==0.3.1) (0.3.1)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.5,>=0.4.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-packs-rag-evaluator==0.3.1) (0.4.2)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (0.12.40)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (1.83.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from pandas->llama-index-packs-rag-evaluator==0.3.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from pandas->llama-index-packs-rag-evaluator==0.3.1) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from pandas->llama-index-packs-rag-evaluator==0.3.1) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from pandas->llama-index-packs-rag-evaluator==0.3.1) (2.0.2)\n",
      "Requirement already satisfied: banks<3,>=2.0.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (2.1.2)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (1.2.18)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (3.2.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (3.9.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (4.13.2)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (4.67.1)\n",
      "Requirement already satisfied: aiosqlite in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (0.21.0)\n",
      "Requirement already satisfied: httpx in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (0.28.1)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (3.12.6)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (2.11.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (9.1.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (2.32.3)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (1.17.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (11.2.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (2025.5.1)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (1.0.8)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (6.0.2)\n",
      "Requirement already satisfied: sqlalchemy[asyncio]>=1.4.49 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (2.0.41)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (1.2.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (0.9.0)\n",
      "Requirement already satisfied: eval-type-backport<0.3,>=0.2.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (0.2.2)\n",
      "Requirement already satisfied: dataclasses-json in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (0.6.7)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (1.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (4.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-packs-rag-evaluator==0.3.1) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (1.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (1.20.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (0.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (6.4.4)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (1.6.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (3.10)\n",
      "Requirement already satisfied: platformdirs in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (4.3.8)\n",
      "Requirement already satisfied: jinja2 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (3.1.6)\n",
      "Requirement already satisfied: griffe in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (1.7.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (1.0.9)\n",
      "Requirement already satisfied: certifi in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (2025.4.26)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (0.16.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (2024.11.6)\n",
      "Requirement already satisfied: joblib in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (1.5.1)\n",
      "Requirement already satisfied: click in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (8.1.8)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (0.4.1)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (2.33.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (2.4.0)\n",
      "Requirement already satisfied: greenlet>=1 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (3.2.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (3.26.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (24.2)\n",
      "Requirement already satisfied: colorama>=0.4 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jinkettyee/.pyenv/versions/3.9.15/envs/great_things/lib/python3.9/site-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai<0.4,>=0.3.0->llama-index-packs-rag-evaluator==0.3.1) (3.0.2)\n",
      "Building wheels for collected packages: llama-index-packs-rag-evaluator\n",
      "  Building wheel for llama-index-packs-rag-evaluator (pyproject.toml): started\n",
      "  Building wheel for llama-index-packs-rag-evaluator (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for llama-index-packs-rag-evaluator: filename=llama_index_packs_rag_evaluator-0.3.1-py3-none-any.whl size=4935 sha256=dad027ca78706e90f46566970ee58c33a8d05a5fec0ef37e09c13ed24e871a8c\n",
      "  Stored in directory: /private/var/folders/nr/6b6zx3jn687ghmtz2_2dw_b40000gn/T/pip-ephem-wheel-cache-41ft15r7/wheels/c5/b3/f2/e8724b5fcdbbb7cd612ab30b90396f09760893fb336af49b78\n",
      "Successfully built llama-index-packs-rag-evaluator\n",
      "Installing collected packages: llama-index-packs-rag-evaluator\n",
      "  Attempting uninstall: llama-index-packs-rag-evaluator\n",
      "    Found existing installation: llama-index-packs-rag-evaluator 0.3.1\n",
      "    Uninstalling llama-index-packs-rag-evaluator-0.3.1:\n",
      "      Successfully uninstalled llama-index-packs-rag-evaluator-0.3.1\n",
      "Successfully installed llama-index-packs-rag-evaluator-0.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Users/jinkettyee/.pyenv/versions/great_things/bin/python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llama_pack import download_llama_pack\n",
    "\n",
    "RagEvaluatorPack = download_llama_pack(\"RagEvaluatorPack\", \"./pack\")\n",
    "\n",
    "# Instantiate RAG Evaluator - input query engine, evaluation dataset, judge LLM & embeddings model\n",
    "rag_evaluator = RagEvaluatorPack(\n",
    "    query_engine=query_engine, \n",
    "    rag_dataset=rag_dataset,\n",
    "    judge_llm=Settings.llm, #use the same llm that we use to create the dataset to judge\n",
    "    embed_model=Settings.embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca4940be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:11,  5.99s/it]\n",
      "2it [00:12,  6.44s/it]\n",
      "2it [00:11,  5.79s/it]\n",
      "2it [00:12,  6.14s/it]\n",
      "2it [00:11,  5.84s/it]\n",
      "2it [00:12,  6.22s/it]\n",
      "2it [00:12,  6.05s/it]\n",
      "2it [00:12,  6.03s/it]\n",
      "2it [00:12,  6.12s/it]\n",
      "2it [00:11,  5.73s/it]\n",
      "2it [00:12,  6.27s/it]\n",
      "2it [00:11,  5.92s/it]\n",
      "2it [00:12,  6.22s/it]\n",
      "2it [00:12,  6.04s/it]\n",
      "2it [00:11,  5.80s/it]\n",
      "2it [00:11,  5.80s/it]\n",
      "2it [00:11,  5.88s/it]\n",
      "2it [00:11,  5.67s/it]\n",
      "2it [00:10,  5.11s/it]\n",
      "2it [00:09,  4.97s/it]\n",
      "2it [00:09,  4.78s/it]\n",
      "2it [00:10,  5.28s/it]\n",
      "2it [00:09,  4.85s/it]\n",
      "2it [00:11,  5.90s/it]\n",
      "2it [00:16,  8.32s/it]\n",
      "2it [00:18,  9.09s/it]\n",
      "2it [00:12,  6.35s/it]\n",
      "1it [00:05,  5.91s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation/benchmarking\n",
    "benchmark_df = rag_evaluator.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed46299c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag                            base_rag\n",
      "metrics                                \n",
      "mean_correctness_score         4.218182\n",
      "mean_relevancy_score           0.981818\n",
      "mean_faithfulness_score        0.836364\n",
      "mean_context_similarity_score  0.952509\n"
     ]
    }
   ],
   "source": [
    "# Review scores\n",
    "print(benchmark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408607d2",
   "metadata": {},
   "source": [
    "*Pretty decent results, reasons likely due to factors associated with evaluation dataset quality, retriever (embedding model) etc.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "great_things",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
